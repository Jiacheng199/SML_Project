{"cells":[{"cell_type":"markdown","metadata":{"id":"CjZ45RladNK6"},"source":["Load data from dataset and preprocess it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4020,"status":"ok","timestamp":1666108245943,"user":{"displayName":"王jiach","userId":"08806092138054227226"},"user_tz":-660},"id":"ey7YEXtMdNK-","outputId":"eebf697f-8c81-4afb-e544-66d03add7798"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/var/folders/9j/ynhzv7dd5yz6hy726z6_pqc00000gn/T/ipykernel_68299/2934194267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcolabcode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColabCode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"]}],"source":["import copy\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","from pandas.io.json import json_normalize\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#read train.json data\n","# orign_train_data = pd.read_json('train.json')\n","\n","orign_train_data = pd.read_json('train.json')\n","\n","#deep copy the original data\n","train_data = copy.deepcopy(orign_train_data)\n","\n","#Get the authors list\n","train_data_authors = train_data['authors']\n","\n","prolific_authors = []\n","coauthors = []\n","#Get the prolific authors list to train the model by removing the coauthors\n","for author in train_data_authors:\n","    p_authors = []\n","    np_authors = []\n","    for name in author:\n","        if name < 100:\n","            p_authors.append(name)\n","        else:\n","            np_authors.append(name)\n","    prolific_authors.append(p_authors)\n","    coauthors.append(np_authors)\n","\n","#add the prolific authors list to the train data\n","train_data['coauthors'] = coauthors\n","train_data['prolific_authors'] = prolific_authors\n","\n","#remove authors in the train data\n","train_data = train_data.drop(['authors'], axis=1)\n","\n","p_a = train_data['prolific_authors']\n","\n","#read test.json data\n","test_data = pd.read_json('test.json')\n","\n","#Pack the prediction result into a csv file\n","def pack_result(result, file_name):\n","    result = pd.DataFrame(result)\n","    #change the 'ID' and 'Predict' column to int32\n","    result['ID'] = result['ID'].astype('int32')\n","    result['Predict'] = result['Predict'].astype('int32')\n","    result.columns = ['ID', 'Predict']\n","    result.to_csv(file_name, index=False)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DNUS4CvIdNLA"},"source":["Split the data into training and testing sets and make the no prolific paper has the -1 label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KAOXsQR2dNLB"},"outputs":[],"source":["\n","#Split the train data into training set and validation set\n","X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['prolific_authors'],axis=1), p_a, test_size=0.2, random_state=42)\n","\n","#get the same index entries in the original train data for training the model\n","X_train_orgin = orign_train_data.loc[X_train.index]\n","\n","#re order the index of the X_train_orgin\n","X_train_orgin = X_train_orgin.reset_index(drop=True)\n","\n","#re order the index of the y_train\n","y_train = y_train.reset_index(drop=True)\n","\n","for i in range(len(y_train)):\n","    # if the author list is empty, add -1 to the list\n","    if len(y_train.iloc[i]) == 0:\n","        y_train.iloc[i].append(-1)\n","\n","for i in range(len(y_test)):\n","    # if the author list is empty, add -1 to the list\n","    if len(y_test.iloc[i]) == 0:\n","        y_test.iloc[i].append(-1)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ehbpbk0ddNLC"},"source":["Functions that used to find information from the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":554,"status":"ok","timestamp":1666167923075,"user":{"displayName":"王jiach","userId":"08806092138054227226"},"user_tz":-660},"id":"R236naESdNLC"},"outputs":[],"source":["all_authors = X_train_orgin['authors']\n","\n","#find authors that author x used to worked with and how many times\n","def find_author_x_work_with(author):\n","    author_x_work_with = {}\n","    author_x_paper = find_author_x_paper(author)\n","    for paper in author_x_paper:\n","        for ath in all_authors.loc[paper]:\n","            if ath != author:\n","                if ath in author_x_work_with:\n","                    author_x_work_with[ath] += 1\n","                else:\n","                    author_x_work_with[ath] = 1\n","    return author_x_work_with\n","\n","#find which paper that author x participated\n","def find_author_x_paper(author):\n","    author_x_paper = []\n","    for i in range(len(all_authors)):\n","        if author in all_authors.loc[i]:\n","            author_x_paper.append(i)\n","    return author_x_paper\n","\n","#find the venue that author published papers\n","def find_author_venue(author):\n","    venues = {}\n","    author_paper = find_author_x_paper(author)\n","    for paper in author_paper:\n","        venue = X_train_orgin.loc[paper]['venue']\n","        if venue in venues:\n","            venues[venue] += 1\n","        else:\n","            venues[venue] = 1\n","    return venues\n","\n","\n","#whether the author x and author y worked with each other\n","def is_author_x_work_with_author_y(author_x, author_y):\n","    author_x_paper = find_author_x_paper(author_x)\n","    author_y_paper = find_author_x_paper(author_y)\n","    for paper in author_x_paper:\n","        if paper in author_y_paper:\n","            return True\n","    return False\n","    \n","#find the prolific authors that the authors worked with (at least one of given authors worked with)\n","def get_authors_prolific_atleast_one(authors):\n","    prolific_authors = []\n","    for author in authors:\n","        prolific_authors.append(find_author_x_work_with(author))\n","    worked_authors = []\n","    for entry in prolific_authors:\n","        worked_authors.extend(entry.keys())\n","\n","    #remove depulicate authors in worked_authors\n","    if worked_authors == []:\n","        return []\n","    worked_authors = list(set(worked_authors))\n","    #get the prolific authors that these authors all worked with\n","    #prolific_authors = set.union(*map(set, prolific_authors))\n","\n","    p_a = []\n","    for author in worked_authors:\n","        if author < 100:\n","            p_a.append(author)\n","    return p_a\n","\n","#Author anylysis\n","# def author_analysis(author):\n","#     author_x_work_with = find_author_x_work_with(author)\n","#     author_x_paper = find_author_x_paper(author)\n","#     author_x_work_with = sorted(author_x_work_with.items(), key=lambda x: x[1], reverse=True)\n","\n","#     print('Author', author, 'has', len(author_x_paper), 'papers')\n","#     print('Author', author, 'has worked with', len(author_x_work_with), 'authors')\n","#     print('Author', author, 'has worked with author - ', author_x_work_with[0][0], 'the most times\\n')\n","#     print('the paper that author', author, 'has participated', author_x_paper)\n","    \n","#     print('--top 5 author that author(include non-prolific authors)', author, 'has worked with the most times--')\n","\n","#     print('the ratio of this author work with', author_x_work_with[0][0], 'is', author_x_work_with[0][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[0][1])\n","#     print('the year that author', author, 'work with author', author_x_work_with[0][0], 'is', year_author_x_work_with_author_y(author, author_x_work_with[0][0]),'\\n')\n","#     print('the ratio of this author work with', author_x_work_with[1][0], 'is', author_x_work_with[1][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[1][1])\n","#     print('the ratio of this author work with', author_x_work_with[2][0], 'is', author_x_work_with[2][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[2][1])\n","#     print('the ratio of this author work with', author_x_work_with[3][0], 'is', author_x_work_with[3][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[3][1])\n","#     print('the ratio of this author work with', author_x_work_with[4][0], 'is', author_x_work_with[4][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[4][1])\n","\n","#     print('Prolific authors that worked with and the time they worked togeth', author, '\\n')\n","#     for i in range(len(author_x_work_with)):\n","#         if author_x_work_with[i][0] < 100:\n","#             print('author id:',author_x_work_with[i][0],'  times:' ,author_x_work_with[i][1])\n","#             print('the venue of author', author, 'work with author', author_x_work_with[i][0], 'is', year_author_x_work_with_author_y(author, author_x_work_with[i][0]),'\\n')\n","            \n"]},{"cell_type":"markdown","metadata":{"id":"cj511v1IdNLE"},"source":["Feature engineering process for the logistic regression model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DN-tXRlrdNLE"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","def feature_engineering(train,test,whole=False):\n","    new_features = {}\n","\n","    #find the ratio of how many coauthors had worked with the prolific author\n","    new_features['work_ratio'] = []\n","\n","    #see if the time of prolific author has worked with the author in the same venue\n","    new_features['same_venue'] = []\n","\n","    #how many coauthors in the given data\n","    new_features['coauthors_count'] = []\n","\n","    #how mant prolific authors that at least one of the coauthors has worked with\n","    new_features['prolific_count'] = []\n","\n","    #whether this prolific author has participated in the same paper with the given coauthors\n","    new_features['is_p_author'] = []\n","\n","\n","    for i in range(len(train)):\n","        authors = train.iloc[i]['coauthors']\n","\n","        #the paper that have only one author and it is also a prolific author will have no coauthors\n","        if len(authors) != 0:\n","\n","            #create two dictionary to store the prolific authors and paper that the prolific authors have participated\n","            worked_authors_dic = {}\n","            published_paper_dic = {}\n","            true_label = test.iloc[i]\n","            #this will skip the paper that have no prolific authors\n","            if true_label == [-1] and not whole:\n","                continue\n","            else:\n","                all_prolific_authors = get_authors_prolific_atleast_one(authors)\n","\n","                #these two features are same for all the possible prolific authors in the given paper\n","                prolific_count = len(all_prolific_authors)\n","                coauthors_count = len(authors)\n","\n","                if all_prolific_authors != []:\n","                    for p_author in all_prolific_authors:\n","                        \n","                        all_p_authors_worked_with = find_author_x_work_with(p_author)\n","                        #copy the keys of the all_p_authors_worked_with\n","                        all_p_authors_worked_with_keys = copy.deepcopy(list(all_p_authors_worked_with.keys()))\n","\n","                        for p_w_author in all_p_authors_worked_with_keys:\n","                            #remove the authors that the author have not worked with\n","                            if p_w_author not in authors:\n","                                all_p_authors_worked_with.pop(p_w_author)\n","\n","                        worked_authors_dic[p_author] = all_p_authors_worked_with\n","                \n","                for wad in worked_authors_dic:\n","                    work_ratio = len(worked_authors_dic[wad])/coauthors_count\n","                    # print('author', wad, 'ratio', work_ratio)\n","\n","                    # venue that prolific author has published paper\n","                    ven = find_author_venue(wad)\n","\n","                    #count the common venue\n","                    same_count = 0\n","                    for v in ven.keys():\n","                        paper_venue = train.iloc[i]['venue']\n","                        if v == paper_venue:\n","                            same_count += ven[v]\n","\n","                    #add the features to the new_featurs\n","                    new_features['work_ratio'].append(work_ratio)\n","                    new_features['same_venue'].append(same_count)\n","                    new_features['coauthors_count'].append(coauthors_count)\n","                    new_features['prolific_count'].append(prolific_count)\n","                    if wad in true_label:\n","                        new_features['is_p_author'].append(1)\n","                    else:\n","                        new_features['is_p_author'].append(0)\n","\n","            print('== ',i,'======================='*5)\n","    \n","    #return the new features as pandas dataframe\n","    new_features = pd.DataFrame(new_features)\n","\n","    #Save the data into a json file\n","    new_features.to_json('new_featurs_10000_20000.json')\n","\n","    return new_features\n","\n","#create a logistic regression model\n","def logistic_regression_model(X_train, y_train, penalty='l2', C=1.0, solver='liblinear', max_iter=100, l1_ratio=None):\n","    logistic_regression = LogisticRegression()\n","    logistic_regression.fit(X_train, y_train)\n","    return logistic_regression\n","\n","# Determine whether the author is the author of the paper using logistic regression\n","def is_prolific_author(author,paper_info):\n","    authors = paper_info['coauthors']\n","    venue = paper_info['venue']\n","    return 1\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUsHs_GA_ynp","outputId":"89fcfdfb-7af0-4e16-8b48-13ef8aa034ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["identifier                                                    0\n","coauthors                            [16336, 1762, 4357, 12564]\n","year                                                         19\n","abstract      [37, 1662, 3207, 10, 33, 2037, 1738, 1642, 155...\n","venue                                                       223\n","title         [3207, 24, 1798, 1738, 37, 2375, 1568, 11, 53,...\n","Name: 0, dtype: object\n","the length of the test data before removing the rows that have no coauthors 5079\n","the length of the test data after removing the rows that have no coauthors 5079\n","========== 0 ==========\n","========== 1 ==========\n","========== 2 ==========\n","========== 3 ==========\n","========== 4 ==========\n","========== 5 ==========\n","========== 6 ==========\n","========== 7 ==========\n","========== 8 ==========\n","========== 9 ==========\n","========== 10 ==========\n","========== 11 ==========\n","========== 12 ==========\n","the author is the possible author of the paper 6\n","ratio 1.0\n","the author is the possible author of the paper 71\n","ratio 1.0\n","the author is the possible author of the paper 73\n","ratio 1.0\n","========== 13 ==========\n","========== 14 ==========\n","========== 15 ==========\n","========== 16 ==========\n","========== 17 ==========\n","========== 18 ==========\n","========== 19 ==========\n","========== 20 ==========\n","========== 21 ==========\n","the author is the possible author of the paper 48\n","ratio 1.0\n","the author is the possible author of the paper 51\n","ratio 1.0\n","========== 22 ==========\n","========== 23 ==========\n","========== 24 ==========\n","the author is the possible author of the paper 75\n","ratio 1.0\n","the author is the possible author of the paper 43\n","ratio 1.0\n","========== 25 ==========\n","========== 26 ==========\n","========== 27 ==========\n","========== 28 ==========\n","the author is the possible author of the paper 31\n","ratio 1.0\n","========== 29 ==========\n","========== 30 ==========\n","the author is the possible author of the paper 10\n","ratio 1.0\n","========== 31 ==========\n","the author is the possible author of the paper 9\n","ratio 1.0\n","the author is the possible author of the paper 82\n","ratio 1.0\n","the author is the possible author of the paper 30\n","ratio 1.0\n","========== 32 ==========\n","========== 33 ==========\n","the author is the possible author of the paper 59\n","ratio 1.0\n","========== 34 ==========\n","========== 35 ==========\n","========== 36 ==========\n","========== 37 ==========\n","========== 38 ==========\n","========== 39 ==========\n","========== 40 ==========\n","========== 41 ==========\n","========== 42 ==========\n","========== 43 ==========\n","========== 44 ==========\n","========== 45 ==========\n","========== 46 ==========\n","========== 47 ==========\n","========== 48 ==========\n","========== 49 ==========\n","========== 50 ==========\n","========== 51 ==========\n","========== 52 ==========\n","========== 53 ==========\n","the author is the possible author of the paper 65\n","ratio 1.0\n","the author is the possible author of the paper 52\n","ratio 1.0\n","========== 54 ==========\n","========== 55 ==========\n","========== 56 ==========\n","========== 57 ==========\n","========== 58 ==========\n","the author is the possible author of the paper 2\n","ratio 1.0\n","the author is the possible author of the paper 6\n","ratio 1.0\n","the author is the possible author of the paper 13\n","ratio 1.0\n","the author is the possible author of the paper 19\n","ratio 1.0\n","the author is the possible author of the paper 23\n","ratio 1.0\n","the author is the possible author of the paper 31\n","ratio 1.0\n","the author is the possible author of the paper 60\n","ratio 1.0\n","the author is the possible author of the paper 71\n","ratio 1.0\n","the author is the possible author of the paper 79\n","ratio 1.0\n","========== 59 ==========\n","========== 60 ==========\n","the author is the possible author of the paper 67\n","ratio 1.0\n","========== 61 ==========\n","the author is the possible author of the paper 73\n","ratio 1.0\n","the author is the possible author of the paper 71\n","ratio 1.0\n","========== 62 ==========\n","========== 63 ==========\n","========== 64 ==========\n","========== 65 ==========\n","========== 66 ==========\n","the author is the possible author of the paper 71\n","ratio 1.0\n","the author is the possible author of the paper 6\n","ratio 1.0\n","========== 67 ==========\n","========== 68 ==========\n","========== 69 ==========\n","the author is the possible author of the paper 75\n","ratio 1.0\n","========== 70 ==========\n","========== 71 ==========\n","========== 72 ==========\n","========== 73 ==========\n","========== 74 ==========\n","========== 75 ==========\n","========== 76 ==========\n","========== 77 ==========\n","========== 78 ==========\n","========== 79 ==========\n","the author is the possible author of the paper 25\n","ratio 1.0\n","========== 80 ==========\n","========== 81 ==========\n","========== 82 ==========\n","========== 83 ==========\n","========== 84 ==========\n","========== 85 ==========\n","========== 86 ==========\n","========== 87 ==========\n","========== 88 ==========\n","========== 89 ==========\n","========== 90 ==========\n","========== 91 ==========\n","========== 92 ==========\n","========== 93 ==========\n","========== 94 ==========\n","the author is the possible author of the paper 89\n","ratio 1.0\n","========== 95 ==========\n","========== 96 ==========\n","========== 97 ==========\n","========== 98 ==========\n","========== 99 ==========\n","the author is the possible author of the paper 65\n","ratio 1.0\n","the author is the possible author of the paper 47\n","ratio 1.0\n","========== 100 ==========\n","========== 101 ==========\n","========== 102 ==========\n","========== 103 ==========\n","========== 104 ==========\n","========== 105 ==========\n","========== 106 ==========\n","========== 107 ==========\n","========== 108 ==========\n","the author is the possible author of the paper 38\n","ratio 1.0\n","========== 109 ==========\n","========== 110 ==========\n","========== 111 ==========\n","========== 112 ==========\n","========== 113 ==========\n","========== 114 ==========\n","========== 115 ==========\n","========== 116 ==========\n","========== 117 ==========\n","the author is the possible author of the paper 84\n","ratio 1.0\n","========== 118 ==========\n","========== 119 ==========\n","the author is the possible author of the paper 37\n","ratio 1.0\n","the author is the possible author of the paper 77\n","ratio 1.0\n","========== 120 ==========\n","========== 121 ==========\n","the author is the possible author of the paper 69\n","ratio 1.0\n","========== 122 ==========\n","the author is the possible author of the paper 58\n","ratio 1.0\n","========== 123 ==========\n","========== 124 ==========\n","========== 125 ==========\n","========== 126 ==========\n","========== 127 ==========\n","========== 128 ==========\n","========== 129 ==========\n","========== 130 ==========\n","========== 131 ==========\n","the author is the possible author of the paper 37\n","ratio 1.0\n","========== 132 ==========\n","========== 133 ==========\n","========== 134 ==========\n","========== 135 ==========\n","========== 136 ==========\n","========== 137 ==========\n","========== 138 ==========\n","the author is the possible author of the paper 60\n","ratio 1.0\n","========== 139 ==========\n","========== 140 ==========\n","========== 141 ==========\n","the author is the possible author of the paper 46\n","ratio 1.0\n","========== 142 ==========\n","========== 143 ==========\n","========== 144 ==========\n","the author is the possible author of the paper 63\n","ratio 1.0\n","========== 145 ==========\n","========== 146 ==========\n","========== 147 ==========\n","the author is the possible author of the paper 79\n","ratio 1.0\n","========== 148 ==========\n","========== 149 ==========\n","========== 150 ==========\n","========== 151 ==========\n","========== 152 ==========\n","========== 153 ==========\n","========== 154 ==========\n","the author is the possible author of the paper 79\n","ratio 1.0\n","========== 155 ==========\n","========== 156 ==========\n","========== 157 ==========\n","========== 158 ==========\n","the author is the possible author of the paper 60\n","ratio 1.0\n","========== 159 ==========\n","========== 160 ==========\n","========== 161 ==========\n","========== 162 ==========\n","========== 163 ==========\n","the author is the possible author of the paper 32\n","ratio 1.0\n","========== 164 ==========\n","========== 165 ==========\n","========== 166 ==========\n","========== 167 ==========\n","========== 168 ==========\n","========== 169 ==========\n","the author is the possible author of the paper 53\n","ratio 1.0\n","========== 170 ==========\n","========== 171 ==========\n","========== 172 ==========\n","========== 173 ==========\n","========== 174 ==========\n","========== 175 ==========\n","========== 176 ==========\n","the author is the possible author of the paper 23\n","ratio 1.0\n","========== 177 ==========\n","the author is the possible author of the paper 37\n","ratio 1.0\n","========== 178 ==========\n","========== 179 ==========\n","========== 180 ==========\n","========== 181 ==========\n","========== 182 ==========\n","========== 183 ==========\n","========== 184 ==========\n","========== 185 ==========\n","========== 186 ==========\n","========== 187 ==========\n","========== 188 ==========\n","========== 189 ==========\n","========== 190 ==========\n","========== 191 ==========\n","========== 192 ==========\n","========== 193 ==========\n","========== 194 ==========\n","========== 195 ==========\n","========== 196 ==========\n","========== 197 ==========\n","========== 198 ==========\n","========== 199 ==========\n","========== 200 ==========\n","========== 201 ==========\n","========== 202 ==========\n","========== 203 ==========\n","========== 204 ==========\n","the author is the possible author of the paper 59\n","ratio 1.0\n","the author is the possible author of the paper 84\n","ratio 1.0\n","========== 205 ==========\n","========== 206 ==========\n","========== 207 ==========\n","========== 208 ==========\n","========== 209 ==========\n","========== 210 ==========\n","========== 211 ==========\n","========== 212 ==========\n","========== 213 ==========\n","========== 214 ==========\n","========== 215 ==========\n","========== 216 ==========\n","========== 217 ==========\n","the author is the possible author of the paper 49\n","ratio 1.0\n","========== 218 ==========\n","========== 219 ==========\n","========== 220 ==========\n","========== 221 ==========\n","========== 222 ==========\n","========== 223 ==========\n","========== 224 ==========\n","========== 225 ==========\n","========== 226 ==========\n","========== 227 ==========\n","the author is the possible author of the paper 10\n","ratio 1.0\n","========== 228 ==========\n","the author is the possible author of the paper 42\n","ratio 1.0\n","the author is the possible author of the paper 36\n","ratio 1.0\n","========== 229 ==========\n","the author is the possible author of the paper 57\n","ratio 0.8333333333333334\n","the author is the possible author of the paper 68\n","ratio 0.8333333333333334\n","the author is the possible author of the paper 98\n","ratio 0.8333333333333334\n","========== 230 ==========\n","========== 231 ==========\n","========== 232 ==========\n","========== 233 ==========\n","========== 234 ==========\n","the author is the possible author of the paper 98\n","ratio 0.8\n","========== 235 ==========\n","========== 236 ==========\n"]}],"source":["from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score\n","\n","new_features_1 = pd.read_json('/content/drive/MyDrive/Colab Notebooks/SML_Project/features_data_0_5000.json')\n","new_features_2 = pd.read_json('/content/drive/MyDrive/Colab Notebooks/SML_Project/features_data_5000_10000.json')\n","new_features_3 = pd.read_json('/content/drive/MyDrive/Colab Notebooks/SML_Project/new_featurs_10000_20000.json')\n","\n","new_features_origin = pd.concat([new_features_1, new_features_2, new_features_3], ignore_index=True)\n","#split the new_features where the is_p_author is 1 and 0\n","new_features_true = new_features_origin[new_features_origin['is_p_author'] == 1]\n","new_features_false = new_features_origin[new_features_origin['is_p_author'] == 0]\n","\n","\n","#add same number of false data to the true data (1:1)\n","training_data_1_1 = pd.concat([new_features_true, new_features_false.sample(n=len(new_features_true), random_state=1)], ignore_index=True)\n","\n","#add twice as many false data to the true data (1:2)\n","training_data_1_2 = pd.concat([new_features_true, new_features_false.sample(n=len(new_features_true)*2, random_state=1)], ignore_index=True)\n","\n","#add half as many false data to the true data (1:0.5)\n","training_data_1_05 = pd.concat([new_features_true, new_features_false.sample(n=int(len(new_features_true)/2), random_state=1)], ignore_index=True)\n","\n","\n","\n","#shuffle the data\n","training_data_1_1 = shuffle(training_data_1_1)\n","training_data_1_2 = shuffle(training_data_1_2)\n","training_data_1_05 = shuffle(training_data_1_05)\n","\n","#training the logistic regression model using the original data\n","X_train_lr = new_features_origin[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n","y_train_lr = new_features_origin['is_p_author']\n","\n","#training the logistic regression model using the training_data_1_1\n","X_train_lr_11 = training_data_1_1[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n","y_train_lr_11 = training_data_1_1['is_p_author']\n","\n","#training the logistic regression model using the training_data_1_2\n","X_train_lr_12 = training_data_1_2[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n","y_train_lr_12 = training_data_1_2['is_p_author']\n","\n","#training the logistic regression model using the training_data_1_05\n","X_train_lr_105 = training_data_1_05[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n","y_train_lr_105 = training_data_1_05['is_p_author']\n","\n","\n","LR = logistic_regression_model(X_train_lr, y_train_lr)\n","LR11 = logistic_regression_model(X_train_lr_11, y_train_lr_11)\n","LR12 = logistic_regression_model(X_train_lr_12, y_train_lr_12)\n","LR105 = logistic_regression_model(X_train_lr_105, y_train_lr_105)\n","\n","print(test_data.iloc[0])\n","#predict the test data\n","prediction = {}\n","prediction['ID'] = []\n","prediction['Predict'] = []\n","\n","#remove all the rows that have no coauthors\n","print('the length of the test data before removing the rows that have no coauthors', len(X_test))\n","X_test = X_test[X_test['coauthors'].map(len) != 0]\n","#remove the same index in the y_test\n","y_test = y_test[X_test.index]\n","print('the length of the test data after removing the rows that have no coauthors', len(X_test))\n","\n","start = 36\n","end = len(test_data)\n","data_to_predict = test_data[start:end]\n","label_to_predict = y_test[start:end]\n","select_model = LR105\n","\n","for i in range(len(data_to_predict)):\n","    authors = data_to_predict.iloc[i]['coauthors']\n","    prediction['ID'].append(i)\n","\n","    #all possible prolific authors of the paper\n","    apa = get_authors_prolific_atleast_one(authors)\n","    print('='*10, i, '='*10)\n","    if apa != []:\n","        prediction_ls = []\n","        for p_author in apa:\n","            ratio = 0\n","            venue_count = 0\n","            coauthors_count = len(authors)\n","            prolific_count = len(apa)\n","\n","            #ratio calculation\n","            all_p_authors_worked_with = find_author_x_work_with(p_author)\n","            all_p_authors_worked_with_keys = copy.deepcopy(list(all_p_authors_worked_with.keys()))\n","            for p_w_author in all_p_authors_worked_with_keys:\n","                #remove the authors that not in the coauthors list\n","                if p_w_author not in authors:\n","                    all_p_authors_worked_with.pop(p_w_author)\n","            ratio = len(all_p_authors_worked_with)/coauthors_count\n","            print(p_author,'ratio: ',ratio)\n","            #venue calculation\n","            paper_venue = data_to_predict.iloc[i]['venue']\n","            p_author_venue = find_author_venue(p_author)\n","            for v in p_author_venue.keys(): #v is the venue, this step count the common venue\n","                if v == paper_venue:\n","                    venue_count += 1\n","            \n","            result = select_model.predict([[ratio, venue_count, coauthors_count, prolific_count]])\n","            if result == [1]:\n","                print('the author is the possible author of the paper', p_author)\n","                print('ratio', ratio)\n","                prediction_ls.append(p_author)\n","\n","        #if no prolific author is a good candidate, then predict no prolific author\n","        if prediction_ls != []: \n","            prediction['Predict'].append(prediction_ls)\n","        else:\n","            prediction['Predict'].append([-1])\n","\n","    else:\n","        prediction['Predict'].append([-1])\n","\n","print(prediction['Predict'])\n","\n","#calculate the accuracy using sklearn\n","prediction = pd.DataFrame(prediction)\n","\n","#save the prediction into a csv file\n","\n","prediction.to_csv('/content/drive/MyDrive/Colab Notebooks/SML_Project/prediction.csv', index=False)\n"]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"4fb6f22b9ea3684756870944b5f3fb664fed3c82022a08a9b2c6685fd75d4878"}}},"nbformat":4,"nbformat_minor":0}
