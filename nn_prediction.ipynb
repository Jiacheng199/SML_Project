{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import copy\n",
    "from numpy import dtype\n",
    "from torch import nn, tensor\n",
    "from torch import optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#load train and test json data\n",
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the authors feature in train data into coauthors and p_authors where p_authors is the author that < 100\n",
    "new_train_data = pd.DataFrame(columns=['id','coauthors','p_author','year','title','abstract','venue'])\n",
    "\n",
    "#using list to store the paper information\n",
    "the_columns = ['id','coauthors','p_author','title','abstract']\n",
    "id_list = []\n",
    "coauthors_list = []\n",
    "p_author_list = []\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "year_list = []\n",
    "venue_list = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    paper_info = train_data.iloc[i]\n",
    "    authors = paper_info['authors']\n",
    "    venue = paper_info['venue']\n",
    "    abstract = paper_info['abstract']\n",
    "    title = paper_info['title']\n",
    "    year = paper_info['year']\n",
    "\n",
    "    coauthors = []\n",
    "    p_authors = []\n",
    "    for author in authors:\n",
    "        if author < 100:\n",
    "            p_authors.append(author)\n",
    "        else:\n",
    "            coauthors.append(author)\n",
    "    \n",
    "    if p_authors == []:\n",
    "        p_authors.append(-1)\n",
    "    \n",
    "    if len(coauthors) != 0:\n",
    "        #not use append to append a list to a dataframe choose a faster way\n",
    "        id_list.append(i)\n",
    "        coauthors_list.append(coauthors)\n",
    "        p_author_list.append(p_authors)\n",
    "        title_list.append(title)\n",
    "        abstract_list.append(abstract)\n",
    "        year_list.append(year)\n",
    "        venue_list.append(venue)\n",
    "\n",
    "#transform the list to dic\n",
    "new_train_data_dic = {'id':id_list,'coauthors':coauthors_list,'p_author':p_author_list,'title':title_list,'abstract':abstract_list,'year':year_list,'venue':venue_list}\n",
    "\n",
    "#transform the dic to dataframe\n",
    "new_train_data = pd.DataFrame(new_train_data_dic)\n",
    "\n",
    "print(new_train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using one hot encoding on the prolific author in the train data using sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "id_list = []\n",
    "coauthors_list = []\n",
    "p_author_list = []\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "year_list = []\n",
    "venue_list = []\n",
    "\n",
    "one_hot_training = ['id','coauthors','p_author','title','abstract','year','venue']\n",
    "for i in range(len(new_train_data)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print('left to process: ',len(new_train_data)-i)\n",
    "    id = new_train_data.iloc[i]['id']\n",
    "    p_author = new_train_data.iloc[i]['p_author']\n",
    "    coauthors = new_train_data.iloc[i]['coauthors']\n",
    "    title = new_train_data.iloc[i]['title']\n",
    "    abstract = new_train_data.iloc[i]['abstract']\n",
    "    year = new_train_data.iloc[i]['year']\n",
    "    venue = new_train_data.iloc[i]['venue']\n",
    "\n",
    "    for p in p_author:\n",
    "        id_list.append(id)\n",
    "        coauthors_list.append(coauthors)\n",
    "        p_author_list.append(p)\n",
    "        title_list.append(title)\n",
    "        abstract_list.append(abstract)\n",
    "        year_list.append(year)\n",
    "        venue_list.append(venue)\n",
    "\n",
    "#create a new dic to store the new data\n",
    "new_train_data_dic = {'id':id_list,'coauthors':coauthors_list,'p_author':p_author_list,'title':title_list,'abstract':abstract_list,'year':year_list,'venue':venue_list}\n",
    "#convert the dic to dataframe\n",
    "new_train_data = pd.DataFrame(new_train_data_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode using pd get_dummies\n",
    "one_hot_training = pd.get_dummies(new_train_data, columns = ['p_author'])\n",
    "#remove all rows that have empty venue\n",
    "one_hot_training = one_hot_training[one_hot_training['venue'] != '']\n",
    "\n",
    "#convert the abstract to doc2vec\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(one_hot_training['abstract'])]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "#convert the abstract to doc2vec\n",
    "abstract_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    #print(model.docvecs[i])\n",
    "    abstract_list.append(model.docvecs[i])\n",
    "#add the abstract to the dataframe\n",
    "one_hot_training['abstract'] = abstract_list\n",
    "\n",
    "#convert the coauthors to 5 demension and fill the empty coauthors with -1\n",
    "coauthors_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    coauthors = one_hot_training.iloc[i]['coauthors']\n",
    "    if len(coauthors) < 5:\n",
    "        coauthors = coauthors + [-1] * (5 - len(coauthors))\n",
    "    else:\n",
    "        #only add the first five coauthors\n",
    "        coauthors = coauthors[0:5]\n",
    "    coauthors_list.append(coauthors)\n",
    "one_hot_training['coauthors'] = coauthors_list\n",
    "\n",
    "#convert the title to doc2vec\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(one_hot_training['title'])]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "#convert the abstract to doc2vec\n",
    "title_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    #print(model.docvecs[i])\n",
    "    title_list.append(model.docvecs[i])\n",
    "one_hot_training['title'] = title_list\n",
    "\n",
    "#convert the venue and year to 5 demension and fill the empty using copy of itself\n",
    "venue_list = []\n",
    "year_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    venue = one_hot_training.iloc[i]['venue']\n",
    "    year = one_hot_training.iloc[i]['year']\n",
    "    venue_list.append([venue] * 5)\n",
    "    year_list.append([year] * 5)\n",
    "one_hot_training['venue'] = venue_list\n",
    "one_hot_training['year'] = year_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "left to process:  20095\n",
      "1000\n",
      "left to process:  19095\n",
      "2000\n",
      "left to process:  18095\n",
      "3000\n",
      "left to process:  17095\n",
      "4000\n",
      "left to process:  16095\n",
      "5000\n",
      "left to process:  15095\n",
      "6000\n",
      "left to process:  14095\n",
      "7000\n",
      "left to process:  13095\n",
      "8000\n",
      "left to process:  12095\n",
      "9000\n",
      "left to process:  11095\n",
      "10000\n",
      "left to process:  10095\n",
      "11000\n",
      "left to process:  9095\n",
      "12000\n",
      "left to process:  8095\n",
      "13000\n",
      "left to process:  7095\n",
      "14000\n",
      "left to process:  6095\n",
      "15000\n",
      "left to process:  5095\n",
      "16000\n",
      "left to process:  4095\n",
      "17000\n",
      "left to process:  3095\n",
      "18000\n",
      "left to process:  2095\n",
      "19000\n",
      "left to process:  1095\n",
      "20000\n",
      "left to process:  95\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLP.train() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m label_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(label_train_c)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39m#training the NN\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#W4sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(tensor_train, label_tensor,\u001b[39m10000\u001b[39;49m,\u001b[39m10\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: MLP.train() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "#split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_training[['coauthors','title','abstract','year','venue']], one_hot_training.drop(['id','coauthors','title','abstract','year','venue'],axis=1), test_size=0.2, random_state=42)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "def convert_to_tensor(training_set):\n",
    "    all_train = []\n",
    "\n",
    "    for i in range(len(training_set)):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print('left to process: ',len(training_set)-i)\n",
    "\n",
    "        coauthors = training_set.iloc[i]['coauthors']\n",
    "        #convert to a float list\n",
    "        coauthors = [float(x) for x in coauthors]\n",
    "\n",
    "        title = training_set.iloc[i]['title']\n",
    "        #convert to a float list\n",
    "        title = [float(x) for x in title]\n",
    "\n",
    "        abstract = training_set.iloc[i]['abstract']\n",
    "        #convert to a float list\n",
    "        abstract = [float(x) for x in abstract]\n",
    "\n",
    "        year = training_set.iloc[i]['year']\n",
    "        #convert to a float list\n",
    "        year = [float(x) for x in year]\n",
    "\n",
    "        venue = training_set.iloc[i]['venue']\n",
    "        #convert to a float list\n",
    "        venue = [float(x) for x in venue]\n",
    "        \n",
    "        all_train.append([coauthors,title,abstract,year,venue])\n",
    "\n",
    "    x = torch.tensor(all_train)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "#create a MLP model with 1 hidden layer and softmax as the output layer\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, train_tensor, label_train_tensor, iterations):\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            for i in range(len(train_tensor)):\n",
    "                optimizer.zero_grad()\n",
    "                output = self(tensor_train[i][0] + tensor_train[i][1] + tensor_train[i][2] + tensor_train[i][3] + tensor_train[i][4])\n",
    "                loss = loss_func(output, label_tensor[i])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            print('Epoch = ',epoch,'Train loss =',train_loss)\n",
    "\n",
    "model = MLP(5,20,101)\n",
    "tensor_train = convert_to_tensor(X_train)\n",
    "\n",
    "#convert the y_train to tensor\n",
    "label_train_c = []\n",
    "for i in range(len(y_train)):\n",
    "    #convert the row to a list\n",
    "    y_train_row = np.array(y_train.iloc[i].tolist())\n",
    "    label_train_c.append(y_train_row)\n",
    "label_tensor = torch.tensor(label_train_c)\n",
    "\n",
    "#training the NN\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(tensor_train,label_tensor,\u001b[39m10000\u001b[39;49m)\n",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb Cell 6\u001b[0m in \u001b[0;36mMLP.train\u001b[0;34m(self, train_tensor, label_train_tensor, iterations)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(train_tensor)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(tensor_train[i][\u001b[39m0\u001b[39;49m] \u001b[39m+\u001b[39;49m tensor_train[i][\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m tensor_train[i][\u001b[39m2\u001b[39;49m] \u001b[39m+\u001b[39;49m tensor_train[i][\u001b[39m3\u001b[39;49m] \u001b[39m+\u001b[39;49m tensor_train[i][\u001b[39m4\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(output, label_tensor[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CV/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb Cell 6\u001b[0m in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CV/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CV/lib/python3.10/site-packages/torch/nn/modules/activation.py:1447\u001b[0m, in \u001b[0;36mLogSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1447\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlog_softmax(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, _stacklevel\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CV/lib/python3.10/site-packages/torch/nn/functional.py:1923\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39mlog_softmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1922\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mlog_softmax(dim)\n\u001b[1;32m   1924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1925\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mlog_softmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "model.train(tensor_train,label_tensor,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the test data to the same shape as train data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9563c21e2246d30c17eee537fc82eb5a8b28d7639b48b7c9a9fa6253fd64ec2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
