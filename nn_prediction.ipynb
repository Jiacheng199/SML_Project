{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import copy\n",
    "from numpy import dtype\n",
    "from torch import nn, tensor\n",
    "from torch import optim\n",
    "from torch import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#load train and test json data\n",
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the authors feature in train data into coauthors and p_authors where p_authors is the author that < 100\n",
    "new_train_data = pd.DataFrame(columns=['id','coauthors','p_author','year','title','abstract','venue'])\n",
    "\n",
    "#using list to store the paper information\n",
    "the_columns = ['id','coauthors','p_author','title','abstract']\n",
    "id_list = []\n",
    "coauthors_list = []\n",
    "p_author_list = []\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "year_list = []\n",
    "venue_list = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    paper_info = train_data.iloc[i]\n",
    "    authors = paper_info['authors']\n",
    "    venue = paper_info['venue']\n",
    "    abstract = paper_info['abstract']\n",
    "    title = paper_info['title']\n",
    "    year = paper_info['year']\n",
    "\n",
    "    coauthors = []\n",
    "    p_authors = []\n",
    "    for author in authors:\n",
    "        if author < 100:\n",
    "            p_authors.append(author)\n",
    "        else:\n",
    "            coauthors.append(author)\n",
    "    \n",
    "    if p_authors == []:\n",
    "        p_authors.append(-1)\n",
    "    \n",
    "    if len(coauthors) != 0:\n",
    "        #not use append to append a list to a dataframe choose a faster way\n",
    "        id_list.append(i)\n",
    "        coauthors_list.append(coauthors)\n",
    "        p_author_list.append(p_authors)\n",
    "        title_list.append(title)\n",
    "        abstract_list.append(abstract)\n",
    "        year_list.append(year)\n",
    "        venue_list.append(venue)\n",
    "\n",
    "#transform the list to dic\n",
    "new_train_data_dic = {'id':id_list,'coauthors':coauthors_list,'p_author':p_author_list,'title':title_list,'abstract':abstract_list,'year':year_list,'venue':venue_list}\n",
    "\n",
    "#transform the dic to dataframe\n",
    "new_train_data = pd.DataFrame(new_train_data_dic)\n",
    "\n",
    "print(new_train_data[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using one hot encoding on the prolific author in the train data using sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "id_list = []\n",
    "coauthors_list = []\n",
    "p_author_list = []\n",
    "title_list = []\n",
    "abstract_list = []\n",
    "year_list = []\n",
    "venue_list = []\n",
    "\n",
    "one_hot_training = ['id','coauthors','p_author','title','abstract','year','venue']\n",
    "for i in range(len(new_train_data)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print('left to process: ',len(new_train_data)-i)\n",
    "    id = new_train_data.iloc[i]['id']\n",
    "    p_author = new_train_data.iloc[i]['p_author']\n",
    "    coauthors = new_train_data.iloc[i]['coauthors']\n",
    "    title = new_train_data.iloc[i]['title']\n",
    "    abstract = new_train_data.iloc[i]['abstract']\n",
    "    year = new_train_data.iloc[i]['year']\n",
    "    venue = new_train_data.iloc[i]['venue']\n",
    "\n",
    "    for p in p_author:\n",
    "        id_list.append(id)\n",
    "        coauthors_list.append(coauthors)\n",
    "        p_author_list.append(p)\n",
    "        title_list.append(title)\n",
    "        abstract_list.append(abstract)\n",
    "        year_list.append(year)\n",
    "        venue_list.append(venue)\n",
    "\n",
    "#create a new dic to store the new data\n",
    "new_train_data_dic = {'id':id_list,'coauthors':coauthors_list,'p_author':p_author_list,'title':title_list,'abstract':abstract_list,'year':year_list,'venue':venue_list}\n",
    "#convert the dic to dataframe\n",
    "new_train_data = pd.DataFrame(new_train_data_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode using pd get_dummies\n",
    "one_hot_training = pd.get_dummies(new_train_data, columns = ['p_author'])\n",
    "#remove all rows that have empty venue\n",
    "one_hot_training = one_hot_training[one_hot_training['venue'] != '']\n",
    "\n",
    "#convert the abstract to doc2vec\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(one_hot_training['abstract'])]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "#convert the abstract to doc2vec\n",
    "abstract_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    #print(model.docvecs[i])\n",
    "    abstract_list.append(model.docvecs[i])\n",
    "#add the abstract to the dataframe\n",
    "one_hot_training['abstract'] = abstract_list\n",
    "\n",
    "#convert the coauthors to 5 demension and fill the empty coauthors with -1\n",
    "coauthors_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    coauthors = one_hot_training.iloc[i]['coauthors']\n",
    "    if len(coauthors) < 5:\n",
    "        coauthors = coauthors + [-1] * (5 - len(coauthors))\n",
    "    else:\n",
    "        #only add the first five coauthors\n",
    "        coauthors = coauthors[0:5]\n",
    "    #append them as integers\n",
    "    coauthors_list.append(coauthors)\n",
    "one_hot_training['coauthors'] = coauthors_list\n",
    "\n",
    "#convert the title to doc2vec\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(one_hot_training['title'])]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "\n",
    "#convert the abstract to doc2vec\n",
    "title_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    #print(model.docvecs[i])\n",
    "    title_list.append(model.docvecs[i])\n",
    "one_hot_training['title'] = title_list\n",
    "\n",
    "#convert the venue and year to 5 demension and fill the empty using copy of itself\n",
    "venue_list = []\n",
    "year_list = []\n",
    "for i in range(len(one_hot_training)):\n",
    "    venue = one_hot_training.iloc[i]['venue']\n",
    "    year = one_hot_training.iloc[i]['year']\n",
    "    venue_list.append([venue] * 5)\n",
    "    year_list.append([year] * 5)\n",
    "    \n",
    "one_hot_training['venue'] = venue_list\n",
    "one_hot_training['year'] = year_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_hot_csv[['coauthors','title','abstract','year','venue']], one_hot_training.drop(['id','coauthors','title','abstract','year','venue'],axis=1), test_size=0.2, random_state=42)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def convert_to_tensor(training_set,device):\n",
    "    all_train = []\n",
    "\n",
    "    for i in range(len(training_set)):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            print('left to process: ',len(training_set)-i)\n",
    "\n",
    "        coauthors = training_set.iloc[i]['coauthors']\n",
    "        coauthors = coauthors.replace('[','')\n",
    "        coauthors = coauthors.replace(']','')\n",
    "        coauthors = coauthors.split()\n",
    "        print(coauthors)\n",
    "        #convert to a float list\n",
    "        coauthors = [float(x) for x in coauthros_list]\n",
    "\n",
    "        title = training_set.iloc[i]['title']\n",
    "        title_list = ast.literal_eval(title)\n",
    "        #convert to a float list\n",
    "        title = [float(x) for x in title_list]\n",
    "\n",
    "        abstract = training_set.iloc[i]['abstract']\n",
    "        #convert to a float list\n",
    "        abstract = [float(x) for x in abstract]\n",
    "\n",
    "        year = training_set.iloc[i]['year']\n",
    "        #convert to a float list\n",
    "        year = [float(x) for x in year]\n",
    "\n",
    "        venue = training_set.iloc[i]['venue']\n",
    "        #convert to a float list\n",
    "        venue = [float(x) for x in venue]\n",
    "        \n",
    "        all_train.append([coauthors,title,abstract,year,venue])\n",
    "\n",
    "    x = torch.tensor(all_train,device=device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "#create a MLP model with 1 hidden layer and softmax as the output layer\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def train(self, train_tensor, label_train_tensor, iterations, batch_size, learning_rate):\n",
    "        #define the loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        #define the optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        #print the target size\n",
    "        print(label_train_tensor.size())\n",
    "        #print the input size\n",
    "        print(train_tensor.size())\n",
    "\n",
    "        #make the label size to be the same as the inpu size\n",
    "        train_tensor = train_tensor[:label_train_tensor.size()[0]]\n",
    "\n",
    "\n",
    "        for i in range(iterations):\n",
    "            #get the batch\n",
    "            batch = train_tensor[i*batch_size:(i+1)*batch_size]\n",
    "            label_batch = label_train_tensor[i*batch_size:(i+1)*batch_size]\n",
    "            #forward pass\n",
    "            output = self.forward(batch)\n",
    "            #calculate the loss\n",
    "            loss = criterion(output, label_batch)\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "            #update the weights\n",
    "            optimizer.step()\n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            #print the loss\n",
    "            if i % 100 == 0:\n",
    "                print('loss: ', loss.item())\n",
    "\n",
    "\n",
    "#duplicate each column's inside data 5 times in the y_train\n",
    "\n",
    "\n",
    "#if GPU is available, it is faster use GPU\n",
    "if cuda.is_available():\n",
    "    \n",
    "    model.cuda()\n",
    "    print('GPU is available')\n",
    "    tensor_train = tensor_train.cuda()\n",
    "    label_train_c = torch.tensor(label_train_c).cuda()\n",
    "\n",
    "label_tensor = torch.tensor(label_train_c)\n",
    "\n",
    "#make label tensor to be the same size as the input tensor\n",
    "label_tensor = label_tensor[:tensor_train.size()[0]]\n",
    "\n",
    "\n",
    "\n",
    "model = MLP(101, 100, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/nn_prediction.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(tensor_train,label_tensor,\u001b[39m10000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor_train' is not defined"
     ]
    }
   ],
   "source": [
    "model.train(tensor_train,label_tensor,10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9563c21e2246d30c17eee537fc82eb5a8b28d7639b48b7c9a9fa6253fd64ec2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
