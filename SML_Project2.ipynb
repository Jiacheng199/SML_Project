{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#read train.json data\n",
    "orign_train_data = pd.read_json('train.json')\n",
    "\n",
    "#deep copy the original data\n",
    "train_data = copy.deepcopy(orign_train_data)\n",
    "\n",
    "#Get the authors list\n",
    "train_data_authors = train_data['authors']\n",
    "\n",
    "prolific_authors = []\n",
    "coauthors = []\n",
    "#Get the prolific authors list to train the model by removing the coauthors\n",
    "for author in train_data_authors:\n",
    "    p_authors = []\n",
    "    np_authors = []\n",
    "    for name in author:\n",
    "        if name < 100:\n",
    "            p_authors.append(name)\n",
    "        else:\n",
    "            np_authors.append(name)\n",
    "    prolific_authors.append(p_authors)\n",
    "    coauthors.append(np_authors)\n",
    "\n",
    "#add the prolific authors list to the train data\n",
    "train_data['coauthors'] = coauthors\n",
    "train_data['prolific_authors'] = prolific_authors\n",
    "\n",
    "#remove authors in the train data\n",
    "train_data = train_data.drop(['authors'], axis=1)\n",
    "\n",
    "p_a = train_data['prolific_authors']\n",
    "\n",
    "#read test.json data\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "#Pack the prediction result into a csv file\n",
    "def pack_result(result, file_name):\n",
    "    result = pd.DataFrame(result)\n",
    "    #change the 'ID' and 'Predict' column to int32\n",
    "    result['ID'] = result['ID'].astype('int32')\n",
    "    result['Predict'] = result['Predict'].astype('int32')\n",
    "    result.columns = ['ID', 'Predict']\n",
    "    result.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets and make the no prolific paper has the -1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the train data into training set and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['prolific_authors'],axis=1), p_a, test_size=0.2, random_state=42)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_train.iloc[i]) == 0:\n",
    "        y_train.iloc[i].append(-1)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_test.iloc[i]) == 0:\n",
    "        y_test.iloc[i].append(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training set into training set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that used to find essential information in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors = orign_train_data['authors']\n",
    "\n",
    "#find authors that author x used to worked with and how many times\n",
    "def find_author_x_work_with(author):\n",
    "    author_x_work_with = {}\n",
    "    author_x_paper = find_author_x_paper(author)\n",
    "    for paper in author_x_paper:\n",
    "        for ath in all_authors.iloc[paper]:\n",
    "            if ath != author:\n",
    "                if ath in author_x_work_with:\n",
    "                    author_x_work_with[ath] += 1\n",
    "                else:\n",
    "                    author_x_work_with[ath] = 1\n",
    "    return author_x_work_with\n",
    "\n",
    "#find which paper that author x participated\n",
    "def find_author_x_paper(author):\n",
    "    author_x_paper = []\n",
    "    for i in range(len(all_authors)):\n",
    "        if author in all_authors.iloc[i]:\n",
    "            author_x_paper.append(i)\n",
    "    return author_x_paper\n",
    "\n",
    "\n",
    "#the years of author x collaborated with author y\n",
    "def year_author_x_work_with_author_y(author_x, author_y):\n",
    "    years = []\n",
    "    author_x_paper = find_author_x_paper(author_x)\n",
    "    author_y_paper = find_author_x_paper(author_y)\n",
    "    for paper in author_x_paper:\n",
    "        if paper in author_y_paper:\n",
    "            years.append(orign_train_data.iloc[paper]['year'])\n",
    "    # remove the duplicate years\n",
    "    years = list(set(years))\n",
    "    return years\n",
    "\n",
    "#get the prolific authors that these authors all worked with\n",
    "def get_prolific_authors(authors):\n",
    "    prolific_authors = []\n",
    "    for author in authors:\n",
    "        prolific_authors.append(find_author_x_work_with(author))\n",
    "    #get the prolific authors that these authors all worked with\n",
    "    prolific_authors = set.intersection(*map(set, prolific_authors))\n",
    "\n",
    "    p_a = []\n",
    "    for author in prolific_authors:\n",
    "        if author < 100:\n",
    "            p_a.append(author)\n",
    "    return p_a\n",
    "\n",
    "#get all sub set of the authors\n",
    "def get_all_sub_set(authors):\n",
    "    all_sub_set = []\n",
    "    for i in range(1, len(authors)):\n",
    "        sub_set = []\n",
    "        for j in range(len(authors)):\n",
    "            sub_set.append(authors[j])\n",
    "            if len(sub_set) == i:\n",
    "                all_sub_set.append(sub_set)\n",
    "                sub_set = []\n",
    "    return all_sub_set\n",
    "\n",
    "#Author anylysis\n",
    "def author_analysis(author):\n",
    "    author_x_work_with = find_author_x_work_with(author)\n",
    "    author_x_paper = find_author_x_paper(author)\n",
    "    author_x_work_with = sorted(author_x_work_with.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print('Author', author, 'has', len(author_x_paper), 'papers')\n",
    "    print('Author', author, 'has worked with', len(author_x_work_with), 'authors')\n",
    "    print('Author', author, 'has worked with author - ', author_x_work_with[0][0], 'the most times\\n')\n",
    "    print('the paper that author', author, 'has participated', author_x_paper)\n",
    "    \n",
    "    print('--top 5 author that author(include non-prolific authors)', author, 'has worked with the most times--')\n",
    "\n",
    "    print('the ratio of this author work with', author_x_work_with[0][0], 'is', author_x_work_with[0][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[0][1])\n",
    "    print('the year that author', author, 'work with author', author_x_work_with[0][0], 'is', year_author_x_work_with_author_y(author, author_x_work_with[0][0]),'\\n')\n",
    "    print('the ratio of this author work with', author_x_work_with[1][0], 'is', author_x_work_with[1][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[1][1])\n",
    "    print('the ratio of this author work with', author_x_work_with[2][0], 'is', author_x_work_with[2][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[2][1])\n",
    "    print('the ratio of this author work with', author_x_work_with[3][0], 'is', author_x_work_with[3][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[3][1])\n",
    "    print('the ratio of this author work with', author_x_work_with[4][0], 'is', author_x_work_with[4][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[4][1])\n",
    "\n",
    "    print('Prolific authors that worked with and the time they worked togeth', author, '\\n')\n",
    "    for i in range(len(author_x_work_with)):\n",
    "        if author_x_work_with[i][0] < 100:\n",
    "            print('author id:',author_x_work_with[i][0],'  times:' ,author_x_work_with[i][1])\n",
    "            print('the venue of author', author, 'work with author', author_x_work_with[i][0], 'is', year_author_x_work_with_author_y(author, author_x_work_with[i][0]),'\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering process for the logistic regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14034, 7787, 17767, 12861, 9924, 7810]\n",
      "get sub set of authors [[14034], [7787], [17767], [12861], [9924], [7810], [14034, 7787], [17767, 12861], [9924, 7810], [14034, 7787, 17767], [12861, 9924, 7810], [14034, 7787, 17767, 12861], [14034, 7787, 17767, 12861, 9924]]\n",
      "sub set [14034]\n",
      "sub_set length 1 [41]\n",
      "\n",
      "sub set [7787]\n",
      "sub_set length 1 []\n",
      "\n",
      "sub set [17767]\n",
      "sub_set length 1 []\n",
      "\n",
      "sub set [12861]\n",
      "sub_set length 1 [44]\n",
      "\n",
      "sub set [9924]\n",
      "sub_set length 1 []\n",
      "\n",
      "sub set [7810]\n",
      "sub_set length 1 [7, 19, 41, 44]\n",
      "\n",
      "sub set [14034, 7787]\n",
      "sub_set length 2 []\n",
      "\n",
      "sub set [17767, 12861]\n",
      "sub_set length 2 []\n",
      "\n",
      "sub set [9924, 7810]\n",
      "sub_set length 2 []\n",
      "\n",
      "sub set [14034, 7787, 17767]\n",
      "sub_set length 3 []\n",
      "\n",
      "sub set [12861, 9924, 7810]\n",
      "sub_set length 3 []\n",
      "\n",
      "sub set [14034, 7787, 17767, 12861]\n",
      "sub_set length 4 []\n",
      "\n",
      "sub set [14034, 7787, 17767, 12861, 9924]\n",
      "sub_set length 5 []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#if the author never work with prolific authors or the recent collaboration is less than 5 years, then predict -1\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "authors = X_test.iloc[1]['coauthors']\n",
    "print(authors)\n",
    "print('get sub set of authors', get_all_sub_set(authors))\n",
    "for sub_set in get_all_sub_set(authors):\n",
    "    print('sub set', sub_set)\n",
    "    print('sub_set length',len(sub_set),get_prolific_authors(sub_set))\n",
    "    print()\n",
    "# answers = y_test.iloc[0]\n",
    "# print(X_test.iloc[0])\n",
    "# print('the authors are', authors)\n",
    "# print(get_prolific_authors(authors))\n",
    "# print('the answer is', answers)\n",
    "\n",
    "def rate_feature_engineering(train,test):\n",
    "    for i in range(len(train)):\n",
    "        authors = train.iloc[i]['coauthors']\n",
    "        #the paper that have only one author and it is also a prolific author will have no coauthors\n",
    "        if len(authors) != 0:\n",
    "            possible_prolific_authors = {}\n",
    "            sub_sets = get_all_sub_set(authors)\n",
    "            for sub_set in sub_sets:\n",
    "                prolific_authors = get_prolific_authors(sub_set)\n",
    "                for prolific_author in prolific_authors:\n",
    "                    p_rate = len(sub_set)/len(authors)\n",
    "                    if prolific_author not in possible_prolific_authors:\n",
    "                        possible_prolific_authors[prolific_author] = p_rate\n",
    "                    else:\n",
    "                        if possible_prolific_authors[prolific_author] < p_rate:\n",
    "                            possible_prolific_authors[prolific_author] = p_rate\n",
    "                    \n",
    "                possible_prolific_authors.append(get_prolific_authors(sub_set))\n",
    "\n",
    "def feature_engineering(train,test):\n",
    "    new_featurs = {}\n",
    "    #how many authors that a prolific author has worked witha in the same paper\n",
    "    new_featurs['rate'] = []\n",
    "    #how many paper that the prolific author has published on the given venue with the respect to the all papers that the prolific author has published\n",
    "    new_featurs['same_venue'] = []\n",
    "    #the count of total collaboration of prolific author with all the coauthors\n",
    "    new_featurs['total_col'] = []\n",
    "    #how many coauthors in the given data\n",
    "    new_featurs['coauthors_count'] = []\n",
    "    new_featurs['is_author'] = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        authors = train.iloc[i]['coauthors']\n",
    "        #the paper that have only one author and it is also a prolific author will have no coauthors\n",
    "        if len(authors) != 0:\n",
    "            possible_prolific_authors = {}\n",
    "            sub_sets = get_all_sub_set(authors)\n",
    "            for sub_set in sub_sets:\n",
    "                prolific_authors = get_prolific_authors(sub_set)\n",
    "                for prolific_author in prolific_authors:\n",
    "                    p_rate = len(sub_set)/len(authors)\n",
    "                    if prolific_author not in possible_prolific_authors:\n",
    "                        possible_prolific_authors[prolific_author] = p_rate\n",
    "                    else:\n",
    "                        if possible_prolific_authors[prolific_author] < p_rate:\n",
    "                            possible_prolific_authors[prolific_author] = p_rate\n",
    "                    \n",
    "                possible_prolific_authors.append(get_prolific_authors(sub_set))\n",
    "\n",
    "    return pd.DataFrame(new_featurs)\n",
    "\n",
    "#create a logistic regression model\n",
    "def logistic_regression_model(X_train, y_train, penalty='l2', C=1.0, solver='liblinear', max_iter=100, l1_ratio=None):\n",
    "    logistic_regression = LogisticRegression(penalty, C, solver, max_iter, l1_ratio)\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    return logistic_regression\n",
    "\n",
    "# Deternqmine whether the author is the author of the paper using logistic regression\n",
    "def is_prolific_author(author,paper_info):\n",
    "    authors = paper_info['coauthors']\n",
    "    venue = paper_info['venue']\n",
    "    \n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_training = feature_engineering(X_train, y_train).drop(['is_author'], axis=1)\n",
    "lr_testing = feature_engineering(X_test, y_test)['is_author']\n",
    "\n",
    "#create the logistic regression model\n",
    "LR = logistic_regression_model(lr_training, lr_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier                                                    0\n",
      "coauthors                            [16336, 1762, 4357, 12564]\n",
      "year                                                         19\n",
      "abstract      [37, 1662, 3207, 10, 33, 2037, 1738, 1642, 155...\n",
      "venue                                                       223\n",
      "title         [3207, 24, 1798, 1738, 37, 2375, 1568, 11, 53,...\n",
      "Name: 0, dtype: object\n",
      "prolific authors [92]\n"
     ]
    }
   ],
   "source": [
    "print(test_data.iloc[0])\n",
    "print('prolific authors', get_prolific_authors(test_data.iloc[0]['coauthors']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9563c21e2246d30c17eee537fc82eb5a8b28d7639b48b7c9a9fa6253fd64ec2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
