{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from dataset and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#read train.json data\n",
    "orign_train_data = pd.read_json('train.json')\n",
    "\n",
    "#deep copy the original data\n",
    "train_data = copy.deepcopy(orign_train_data)\n",
    "\n",
    "#Get the authors list\n",
    "train_data_authors = train_data['authors']\n",
    "\n",
    "prolific_authors = []\n",
    "coauthors = []\n",
    "#Get the prolific authors list to train the model by removing the coauthors\n",
    "for author in train_data_authors:\n",
    "    p_authors = []\n",
    "    np_authors = []\n",
    "    for name in author:\n",
    "        if name < 100:\n",
    "            p_authors.append(name)\n",
    "        else:\n",
    "            np_authors.append(name)\n",
    "    prolific_authors.append(p_authors)\n",
    "    coauthors.append(np_authors)\n",
    "\n",
    "#add the prolific authors list to the train data\n",
    "train_data['coauthors'] = coauthors\n",
    "train_data['prolific_authors'] = prolific_authors\n",
    "\n",
    "#remove authors in the train data\n",
    "train_data = train_data.drop(['authors'], axis=1)\n",
    "\n",
    "p_a = train_data['prolific_authors']\n",
    "\n",
    "#read test.json data\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "#Pack the prediction result into a csv file\n",
    "def pack_result(result, file_name):\n",
    "    result = pd.DataFrame(result)\n",
    "    #change the 'ID' and 'Predict' column to int32\n",
    "    result['ID'] = result['ID'].astype('int32')\n",
    "    result['Predict'] = result['Predict'].astype('int32')\n",
    "    result.columns = ['ID', 'Predict']\n",
    "    result.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets and make the no prolific paper has the -1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the train data into training set and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['prolific_authors'],axis=1), p_a, test_size=0.2, random_state=42)\n",
    "\n",
    "#get the same index entries in the original train data for training the model\n",
    "X_train_orgin = orign_train_data.loc[X_train.index]\n",
    "\n",
    "#re order the index of the X_train_orgin\n",
    "X_train_orgin = X_train_orgin.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the y_train\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the X_test\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the y_test\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_train.iloc[i]) == 0:\n",
    "        y_train.iloc[i].append(-1)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_test.iloc[i]) == 0:\n",
    "        y_test.iloc[i].append(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that used to find information from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors = X_train_orgin['authors']\n",
    "\n",
    "#find authors that author x used to worked with and how many times\n",
    "def find_author_x_work_with(author):\n",
    "    author_x_work_with = {}\n",
    "    author_x_paper = find_author_x_paper(author)\n",
    "    for paper in author_x_paper:\n",
    "        for ath in all_authors.loc[paper]:\n",
    "            if ath != author:\n",
    "                if ath in author_x_work_with:\n",
    "                    author_x_work_with[ath] += 1\n",
    "                else:\n",
    "                    author_x_work_with[ath] = 1\n",
    "    return author_x_work_with\n",
    "\n",
    "#find which paper that author x participated\n",
    "def find_author_x_paper(author):\n",
    "    author_x_paper = []\n",
    "    for i in range(len(all_authors)):\n",
    "        if author in all_authors.loc[i]:\n",
    "            author_x_paper.append(i)\n",
    "    return author_x_paper\n",
    "\n",
    "#find the venue that author published papers\n",
    "def find_author_venue(author):\n",
    "    venues = {}\n",
    "    author_paper = find_author_x_paper(author)\n",
    "    for paper in author_paper:\n",
    "        venue = X_train_orgin.loc[paper]['venue']\n",
    "        if venue in venues:\n",
    "            venues[venue] += 1\n",
    "        else:\n",
    "            venues[venue] = 1\n",
    "    return venues\n",
    "\n",
    "\n",
    "#whether the author x and author y worked with each other\n",
    "def is_author_x_work_with_author_y(author_x, author_y):\n",
    "    author_x_paper = find_author_x_paper(author_x)\n",
    "    author_y_paper = find_author_x_paper(author_y)\n",
    "    for paper in author_x_paper:\n",
    "        if paper in author_y_paper:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "#find the prolific authors that the authors worked with (at least one of given authors worked with)\n",
    "def get_authors_prolific_atleast_one(authors):\n",
    "    prolific_authors = []\n",
    "    for author in authors:\n",
    "        prolific_authors.append(find_author_x_work_with(author))\n",
    "    #get the prolific authors that these authors all worked with\n",
    "    prolific_authors = set.union(*map(set, prolific_authors))\n",
    "\n",
    "    p_a = []\n",
    "    for author in prolific_authors:\n",
    "        if author < 100:\n",
    "            p_a.append(author)\n",
    "    return p_a\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering process for the logistic regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 19, 23, 38, 51, 57, 81]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def feature_engineering(train,test):\n",
    "    new_featurs = {}\n",
    "\n",
    "    #find the ratio of how many coauthors had worked with the prolific author\n",
    "    new_featurs['work_ratio'] = []\n",
    "\n",
    "    #see if the time of prolific author has worked with the author in the same venue\n",
    "    new_featurs['same_venue'] = []\n",
    "\n",
    "    #how many coauthors in the given data\n",
    "    new_featurs['coauthors_count'] = []\n",
    "\n",
    "    #how mant prolific authors that at least one of the coauthors has worked with\n",
    "    new_featurs['prolific_count'] = []\n",
    "\n",
    "    #whether this prolific author has participated in the same paper with the given coauthors\n",
    "    new_featurs['is_p_author'] = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        authors = train.iloc[i]['coauthors']\n",
    "\n",
    "        #the paper that have only one author and it is also a prolific author will have no coauthors\n",
    "        if len(authors) != 0:\n",
    "\n",
    "            #create two dictionary to store the prolific authors and paper that the prolific authors have participated\n",
    "            worked_authors_dic = {}\n",
    "            published_paper_dic = {}\n",
    "            true_lable = test.iloc[i]\n",
    "\n",
    "            all_prolific_authors = get_authors_prolific_atleast_one(authors)\n",
    "\n",
    "            #these two features are same for all the possible prolific authors in the given paper\n",
    "            prolific_count = len(all_prolific_authors)\n",
    "            coauthors_count = len(authors)\n",
    "\n",
    "            if all_prolific_authors != []:\n",
    "                for p_author in all_prolific_authors:\n",
    "                    \n",
    "                    all_p_authors_worked_with = find_author_x_work_with(p_author)\n",
    "                    #copy the keys of the all_p_authors_worked_with\n",
    "                    all_p_authors_worked_with_keys = copy.deepcopy(list(all_p_authors_worked_with.keys()))\n",
    "\n",
    "                    for p_w_author in all_p_authors_worked_with_keys:\n",
    "                        #remove the authors that the author have not worked with\n",
    "                        if p_w_author not in authors:\n",
    "                            all_p_authors_worked_with.pop(p_w_author)\n",
    "\n",
    "                    worked_authors_dic[p_author] = all_p_authors_worked_with\n",
    "            \n",
    "            for wad in worked_authors_dic:\n",
    "                work_ratio = len(worked_authors_dic[wad])/coauthors_count\n",
    "                # print('author', wad, 'ratio', work_ratio)\n",
    "\n",
    "                # venue that prolific author has published paper\n",
    "                ven = find_author_venue(wad)\n",
    "\n",
    "                #count the common venue\n",
    "                same_count = 0\n",
    "                for v in ven.keys():\n",
    "                    paper_venue = train.iloc[i]['venue']\n",
    "                    if v == paper_venue:\n",
    "                        same_count += ven[v]\n",
    "\n",
    "                #add the features to the new_featurs\n",
    "                new_featurs['work_ratio'].append(work_ratio)\n",
    "                new_featurs['same_venue'].append(same_count)\n",
    "                new_featurs['coauthors_count'].append(coauthors_count)\n",
    "                new_featurs['prolific_count'].append(prolific_count)\n",
    "                if wad in true_lable:\n",
    "                    new_featurs['is_p_author'].append(1)\n",
    "                else:\n",
    "                    new_featurs['is_p_author'].append(0)\n",
    "\n",
    "            # print('all_prolific_authors', all_prolific_authors)\n",
    "            # print('the coauthors of paper', i, 'are', authors)\n",
    "            # print('worked authors dic', worked_authors_dic)\n",
    "            \n",
    "            print('== ',i,'======================='*5)\n",
    "    \n",
    "    #return the new features as pandas dataframe\n",
    "    new_featurs = pd.DataFrame(new_featurs)\n",
    "\n",
    "    #Save the data into a json file\n",
    "    new_featurs.to_json('new_featurs.json')\n",
    "\n",
    "    return new_featurs\n",
    "\n",
    "#create a logistic regression model\n",
    "def logistic_regression_model(X_train, y_train, penalty='l2', C=1.0, solver='liblinear', max_iter=100, l1_ratio=None):\n",
    "    logistic_regression = LogisticRegression()\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    return logistic_regression\n",
    "\n",
    "# Determine whether the author is the author of the paper using logistic regression\n",
    "def is_prolific_author(author,paper_info):\n",
    "    authors = paper_info['coauthors']\n",
    "    venue = paper_info['venue']\n",
    "    all_prolicific_authors = get_authors_prolific_atleast_one(authors)\n",
    "    if all_prolicific_authors != []:\n",
    "        for p_author in all_prolicific_authors:\n",
    "            #get the ratio of how many coauthors had worked with the prolific author\n",
    "\n",
    "            return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# feature_engineering(X_train[0:1000], y_train[0:1000])\n",
    "# print(X_train_orgin)\n",
    "print(get_authors_prolific_atleast_one([91]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of the test data before removing the rows that have no coauthors 5079\n",
      "the length of the test data after removing the rows that have no coauthors 5079\n"
     ]
    }
   ],
   "source": [
    "new_featurs = pd.read_json('features_data.json')\n",
    "\n",
    "#print the first 5 rows of the features\n",
    "new_featurs.head()\n",
    "\n",
    "#traing the logistic regression model\n",
    "traing_features = np.array(new_featurs[['work_ratio','same_venue','coauthors_count','prolific_count']])\n",
    "traing_labels = np.array(new_featurs['is_p_author'])\n",
    "LR = logistic_regression_model(traing_features, traing_labels)\n",
    "\n",
    "#predict the test data\n",
    "prediction = {}\n",
    "prediction['ID'] = []\n",
    "prediction['Predict'] = []\n",
    "\n",
    "#remove all the rows that have no coauthors\n",
    "print('the length of the test data before removing the rows that have no coauthors', len(X_test))\n",
    "X_test = X_test[X_test['coauthors'].map(len) != 0]\n",
    "#remove the same index in the y_test\n",
    "y_test = y_test[X_test.index]\n",
    "print('the length of the test data after removing the rows that have no coauthors', len(X_test))\n",
    "    \n",
    "# for i in range(len(X_test[0:10])):\n",
    "#     authors = X_test.iloc[i]['coauthors']\n",
    "#     paper_info = X_test.iloc[i]\n",
    "#     prediction['ID'].append(i)\n",
    "#     apa = get_authors_prolific_atleast_one(authors)\n",
    "#     if apa != []:\n",
    "#         prediction['Predict'].append(1)\n",
    "#     else:\n",
    "#         prediction['Predict'].append(-1)\n",
    "\n",
    "#save the prediction into a csv file\n",
    "# prediction = pd.DataFrame(prediction)\n",
    "# prediction.to_csv('prediction.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year                                           abstract venue  \\\n",
      "0      7  [1623, 1621, 1700, 1543, 33, 2155, 1669, 1916,...   129   \n",
      "1     10  [40, 1542, 1691, 2380, 1604, 1839, 1553, 1535,...    44   \n",
      "2      4  [46, 1605, 1691, 1557, 2227, 1535, 6, 2036, 7,...   195   \n",
      "3     10  [3892, 1803, 1532, 11, 1650, 2414, 1692, 1866,...    44   \n",
      "4     17  [1529, 2264, 1542, 1529, 3204, 3448, 45, 1559,...     9   \n",
      "6     11  [1532, 2783, 1905, 1613, 3743, 1535, 2079, 232...     5   \n",
      "7     13  [40, 1542, 1691, 2449, 1535, 2992, 1543, 33, 1...    42   \n",
      "8      7  [37, 2227, 33, 1642, 1586, 1720, 1703, 1814, 3...    10   \n",
      "9      5  [37, 2495, 1535, 1616, 3296, 2254, 43, 1553, 3...    32   \n",
      "10    15  [37, 1558, 1671, 2526, 1594, 46, 3001, 1575, 6...     9   \n",
      "11    16  [37, 2611, 3298, 2182, 2210, 1525, 1535, 1624,...    28   \n",
      "12     9  [1731, 2610, 1543, 1535, 37, 34, 16, 12, 14, 1...    53   \n",
      "13    16  [53, 1546, 1752, 1682, 2565, 1659, 3713, 1543,...    10   \n",
      "14    16  [37, 2087, 1608, 1772, 1630, 2216, 3041, 1546,...    31   \n",
      "15    14  [46, 1605, 1691, 10, 1557, 2227, 1672, 2669, 1...         \n",
      "16     7  [37, 1662, 1810, 1811, 1772, 2096, 1546, 1758,...    34   \n",
      "17     6  [46, 1605, 1691, 10, 1557, 2227, 1604, 3247, 1...    10   \n",
      "18    12  [40, 1542, 1691, 3946, 1649, 1988, 1535, 1793,...   174   \n",
      "19     9  [1606, 1645, 52, 1811, 1700, 6, 7, 1839, 1613,...    30   \n",
      "20     2  [56, 1687, 1864, 3162, 3426, 1655, 2316, 2155,...   244   \n",
      "\n",
      "                                                title  \\\n",
      "0   [40, 1745, 1754, 1553, 2984, 1811, 1700, 1704,...   \n",
      "1   [1578, 1528, 11, 3552, 2123, 2269, 1538, 1560,...   \n",
      "2   [46, 1731, 1536, 38, 53, 1546, 1752, 47, 1751,...   \n",
      "3   [45, 1559, 2179, 40, 1621, 1536, 3591, 37, 208...   \n",
      "4   [1529, 2264, 24, 1539, 1740, 1529, 1589, 1629,...   \n",
      "6   [52, 1918, 57, 1527, 2085, 1719, 1846, 1745, 1...   \n",
      "7   [1708, 1535, 37, 34, 1546, 1560, 1525, 33, 47,...   \n",
      "8   [1615, 1527, 1649, 1547, 1543, 24, 3368, 47, 2...   \n",
      "9   [1623, 1621, 50, 1620, 1632, 1554, 1564, 1661,...   \n",
      "10  [37, 1558, 1671, 2526, 1594, 1723, 46, 3001, 1...   \n",
      "11  [4284, 11, 2965, 46, 1841, 2225, 1547, 1543, 1...   \n",
      "12  [47, 1574, 1541, 1854, 52, 1918, 57, 1543, 324...   \n",
      "13  [2123, 2269, 1538, 1541, 2230, 1560, 1684, 165...   \n",
      "14  [11, 1529, 41, 1614, 51, 1539, 1615, 1966, 11,...   \n",
      "15  [47, 1730, 1575, 1538, 46, 36, 1529, 1651, 192...   \n",
      "16  [47, 1945, 2014, 3207, 1538, 46, 1624, 1547, 5...   \n",
      "17  [57, 2823, 48, 1559, 1623, 1546, 41, 3188, 166...   \n",
      "18  [1542, 2048, 1852, 2298, 1553, 48, 1618, 1588,...   \n",
      "19  [1716, 1528, 51, 1892, 48, 1559, 1623, 1540, 1...   \n",
      "20  [1635, 2106, 51, 40, 1539, 45, 1559, 1565, 47,...   \n",
      "\n",
      "                                            coauthors  \n",
      "0                                        [6639, 6354]  \n",
      "1             [14034, 7787, 17767, 12861, 9924, 7810]  \n",
      "2                                [16988, 2308, 20183]  \n",
      "3                               [17256, 15052, 12164]  \n",
      "4                                               [646]  \n",
      "6                                       [12803, 8177]  \n",
      "7                                [4523, 15805, 12132]  \n",
      "8        [6262, 5862, 18456, 8276, 15353, 4407, 1842]  \n",
      "9                                      [20133, 21090]  \n",
      "10                          [19761, 7893, 9307, 6402]  \n",
      "11                                       [6120, 9515]  \n",
      "12                          [162, 10022, 2332, 18801]  \n",
      "13                                              [719]  \n",
      "14                                     [15008, 11402]  \n",
      "15                     [5259, 7023, 12467, 9492, 635]  \n",
      "16                                      [5249, 10883]  \n",
      "17                                             [7672]  \n",
      "18                                 [20020, 5107, 366]  \n",
      "19  [205, 15734, 17889, 727, 16918, 17961, 12416, ...  \n",
      "20                                      [4286, 10812]  \n",
      "0     [-1]\n",
      "1     [-1]\n",
      "2     [-1]\n",
      "3     [-1]\n",
      "4     [-1]\n",
      "6     [57]\n",
      "7     [-1]\n",
      "8     [-1]\n",
      "9      [2]\n",
      "10    [88]\n",
      "11    [-1]\n",
      "12    [-1]\n",
      "13    [31]\n",
      "14    [-1]\n",
      "15    [75]\n",
      "16    [-1]\n",
      "17    [-1]\n",
      "18    [-1]\n",
      "19    [78]\n",
      "20     [5]\n",
      "Name: prolific_authors, dtype: object\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "print(X_test[0:20])\n",
    "print(y_test[0:20])\n",
    "\n",
    "print(get_authors_prolific_atleast_one([4286, 10812]))\n",
    "#要解决的问题，由于X_train被重新排序，所以对测试数据进行预测的时候，需要原来的index，这是由于关键方程 get_author_paper 的原理导致的\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb6f22b9ea3684756870944b5f3fb664fed3c82022a08a9b2c6685fd75d4878"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
