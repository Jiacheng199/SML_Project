{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from dataset and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#read train.json data\n",
    "orign_train_data = pd.read_json('train.json')\n",
    "\n",
    "#deep copy the original data\n",
    "train_data = copy.deepcopy(orign_train_data)\n",
    "\n",
    "#Get the authors list\n",
    "train_data_authors = train_data['authors']\n",
    "\n",
    "prolific_authors = []\n",
    "coauthors = []\n",
    "#Get the prolific authors list to train the model by removing the coauthors\n",
    "for author in train_data_authors:\n",
    "    p_authors = []\n",
    "    np_authors = []\n",
    "    for name in author:\n",
    "        if name < 100:\n",
    "            p_authors.append(name)\n",
    "        else:\n",
    "            np_authors.append(name)\n",
    "    prolific_authors.append(p_authors)\n",
    "    coauthors.append(np_authors)\n",
    "\n",
    "#add the prolific authors list to the train data\n",
    "train_data['coauthors'] = coauthors\n",
    "train_data['prolific_authors'] = prolific_authors\n",
    "\n",
    "#remove authors in the train data\n",
    "train_data = train_data.drop(['authors'], axis=1)\n",
    "\n",
    "p_a = train_data['prolific_authors']\n",
    "\n",
    "#read test.json data\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "#Pack the prediction result into a csv file\n",
    "def pack_result(result, file_name):\n",
    "    result = pd.DataFrame(result)\n",
    "    #change the 'ID' and 'Predict' column to int32\n",
    "    result['ID'] = result['ID'].astype('int32')\n",
    "    result['Predict'] = result['Predict'].astype('int32')\n",
    "    result.columns = ['ID', 'Predict']\n",
    "    result.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets and make the no prolific paper has the -1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the train data into training set and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['prolific_authors'],axis=1), p_a, test_size=0.2, random_state=42)\n",
    "\n",
    "#get the same index entries in the original train data for training the model\n",
    "X_train_orgin = orign_train_data.loc[X_train.index]\n",
    "\n",
    "#re order the index of the X_train_orgin\n",
    "X_train_orgin = X_train_orgin.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the y_train\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_train.iloc[i]) == 0:\n",
    "        y_train.iloc[i].append(-1)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_test.iloc[i]) == 0:\n",
    "        y_test.iloc[i].append(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that used to find information from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors = X_train_orgin['authors']\n",
    "\n",
    "#find authors that author x used to worked with and how many times\n",
    "def find_author_x_work_with(author):\n",
    "    author_x_work_with = {}\n",
    "    author_x_paper = find_author_x_paper(author)\n",
    "    for paper in author_x_paper:\n",
    "        for ath in all_authors.loc[paper]:\n",
    "            if ath != author:\n",
    "                if ath in author_x_work_with:\n",
    "                    author_x_work_with[ath] += 1\n",
    "                else:\n",
    "                    author_x_work_with[ath] = 1\n",
    "    return author_x_work_with\n",
    "\n",
    "#find which paper that author x participated\n",
    "def find_author_x_paper(author):\n",
    "    author_x_paper = []\n",
    "    for i in range(len(all_authors)):\n",
    "        if author in all_authors.loc[i]:\n",
    "            author_x_paper.append(i)\n",
    "    return author_x_paper\n",
    "\n",
    "#find the venue that author published papers\n",
    "def find_author_venue(author):\n",
    "    venues = {}\n",
    "    author_paper = find_author_x_paper(author)\n",
    "    for paper in author_paper:\n",
    "        venue = X_train_orgin.loc[paper]['venue']\n",
    "        if venue in venues:\n",
    "            venues[venue] += 1\n",
    "        else:\n",
    "            venues[venue] = 1\n",
    "    return venues\n",
    "\n",
    "\n",
    "#whether the author x and author y worked with each other\n",
    "def is_author_x_work_with_author_y(author_x, author_y):\n",
    "    author_x_paper = find_author_x_paper(author_x)\n",
    "    author_y_paper = find_author_x_paper(author_y)\n",
    "    for paper in author_x_paper:\n",
    "        if paper in author_y_paper:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "#find the prolific authors that the authors worked with (at least one of given authors worked with)\n",
    "def get_authors_prolific_atleast_one(authors):\n",
    "    prolific_authors = []\n",
    "    for author in authors:\n",
    "        prolific_authors.append(find_author_x_work_with(author))\n",
    "    #get the prolific authors that these authors all worked with\n",
    "    prolific_authors = set.union(*map(set, prolific_authors))\n",
    "\n",
    "    p_a = []\n",
    "    for author in prolific_authors:\n",
    "        if author < 100:\n",
    "            p_a.append(author)\n",
    "    return p_a\n",
    "\n",
    "#Author anylysis\n",
    "# def author_analysis(author):\n",
    "#     author_x_work_with = find_author_x_work_with(author)\n",
    "#     author_x_paper = find_author_x_paper(author)\n",
    "#     author_x_work_with = sorted(author_x_work_with.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     print('Author', author, 'has', len(author_x_paper), 'papers')\n",
    "#     print('Author', author, 'has worked with', len(author_x_work_with), 'authors')\n",
    "#     print('Author', author, 'has worked with author - ', author_x_work_with[0][0], 'the most times\\n')\n",
    "#     print('the paper that author', author, 'has participated', author_x_paper)\n",
    "    \n",
    "#     print('--top 5 author that author(include non-prolific authors)', author, 'has worked with the most times--')\n",
    "\n",
    "#     print('the ratio of this author work with', author_x_work_with[0][0], 'is', author_x_work_with[0][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[0][1])\n",
    "#     print('the year that author', author, 'work with author', author_x_work_with[0][0], 'is', year_author_x_work_with_author_y(author, author_x_work_with[0][0]),'\\n')\n",
    "#     print('the ratio of this author work with', author_x_work_with[1][0], 'is', author_x_work_with[1][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[1][1])\n",
    "#     print('the ratio of this author work with', author_x_work_with[2][0], 'is', author_x_work_with[2][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[2][1])\n",
    "#     print('the ratio of this author work with', author_x_work_with[3][0], 'is', author_x_work_with[3][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[3][1])\n",
    "#     print('the ratio of this author work with', author_x_work_with[4][0], 'is', author_x_work_with[4][1]/len(author_x_paper), 'the number of collaboration is', author_x_work_with[4][1])\n",
    "\n",
    "#     print('Prolific authors that worked with and the time they worked togeth', author, '\\n')\n",
    "#     for i in range(len(author_x_work_with)):\n",
    "#         if author_x_work_with[i][0] < 100:\n",
    "#             print('author id:',author_x_work_with[i][0],'  times:' ,author_x_work_with[i][1])\n",
    "#             print('the venue of author', author, 'work with author', author_x_work_with[i][0], 'is', year_author_x_work_with_author_y(author, author_x_work_with[i][0]),'\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering process for the logistic regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==  0 ===================================================================================================================\n",
      "==  1 ===================================================================================================================\n",
      "==  2 ===================================================================================================================\n",
      "==  3 ===================================================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/ynhzv7dd5yz6hy726z6_pqc00000gn/T/ipykernel_48924/2232616978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0mfeature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/9j/ynhzv7dd5yz6hy726z6_pqc00000gn/T/ipykernel_48924/2232616978.py\u001b[0m in \u001b[0;36mfeature_engineering\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrue_lable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mall_prolific_authors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_authors_prolific_atleast_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#these two features are same for all the possible prolific authors in the given paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9j/ynhzv7dd5yz6hy726z6_pqc00000gn/T/ipykernel_48924/4113106262.py\u001b[0m in \u001b[0;36mget_authors_prolific_atleast_one\u001b[0;34m(authors)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprolific_authors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mprolific_authors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_author_x_work_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;31m#get the prolific authors that these authors all worked with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprolific_authors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprolific_authors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9j/ynhzv7dd5yz6hy726z6_pqc00000gn/T/ipykernel_48924/4113106262.py\u001b[0m in \u001b[0;36mfind_author_x_work_with\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_author_x_work_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauthor_x_work_with\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mauthor_x_paper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_author_x_paper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthor_x_paper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0math\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_authors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9j/ynhzv7dd5yz6hy726z6_pqc00000gn/T/ipykernel_48924/4113106262.py\u001b[0m in \u001b[0;36mfind_author_x_paper\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mauthor_x_paper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_authors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_authors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mauthor_x_paper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauthor_x_paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \"\"\"\n\u001b[1;32m    348\u001b[0m     \u001b[0mEvaluate\u001b[0m \u001b[0mpossibly\u001b[0m \u001b[0mcallable\u001b[0m \u001b[0minput\u001b[0m \u001b[0musing\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py\u001b[0m in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_stopped'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3.x has this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mis_thread_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def feature_engineering(train,test):\n",
    "    new_featurs = {}\n",
    "\n",
    "    #find the ratio of how many coauthors had worked with the prolific author\n",
    "    new_featurs['work_ratio'] = []\n",
    "\n",
    "    #see if the time of prolific author has worked with the author in the same venue\n",
    "    new_featurs['same_venue'] = []\n",
    "\n",
    "    #how many coauthors in the given data\n",
    "    new_featurs['coauthors_count'] = []\n",
    "\n",
    "    #how mant prolific authors that at least one of the coauthors has worked with\n",
    "    new_featurs['prolific_count'] = []\n",
    "\n",
    "    #whether this prolific author has participated in the same paper with the given coauthors\n",
    "    new_featurs['is_p_author'] = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        authors = train.iloc[i]['coauthors']\n",
    "\n",
    "        #the paper that have only one author and it is also a prolific author will have no coauthors\n",
    "        if len(authors) != 0:\n",
    "\n",
    "            #create two dictionary to store the prolific authors and paper that the prolific authors have participated\n",
    "            worked_authors_dic = {}\n",
    "            published_paper_dic = {}\n",
    "            true_lable = test.iloc[i]\n",
    "\n",
    "            all_prolific_authors = get_authors_prolific_atleast_one(authors)\n",
    "\n",
    "            #these two features are same for all the possible prolific authors in the given paper\n",
    "            prolific_count = len(all_prolific_authors)\n",
    "            coauthors_count = len(authors)\n",
    "\n",
    "            if all_prolific_authors != []:\n",
    "                for p_author in all_prolific_authors:\n",
    "                    \n",
    "                    all_p_authors_worked_with = find_author_x_work_with(p_author)\n",
    "                    #copy the keys of the all_p_authors_worked_with\n",
    "                    all_p_authors_worked_with_keys = copy.deepcopy(list(all_p_authors_worked_with.keys()))\n",
    "\n",
    "                    for p_w_author in all_p_authors_worked_with_keys:\n",
    "                        #remove the authors that the author have not worked with\n",
    "                        if p_w_author not in authors:\n",
    "                            all_p_authors_worked_with.pop(p_w_author)\n",
    "\n",
    "                    worked_authors_dic[p_author] = all_p_authors_worked_with\n",
    "            \n",
    "            for wad in worked_authors_dic:\n",
    "                work_ratio = len(worked_authors_dic[wad])/coauthors_count\n",
    "                # print('author', wad, 'ratio', work_ratio)\n",
    "\n",
    "                # venue that prolific author has published paper\n",
    "                ven = find_author_venue(wad)\n",
    "\n",
    "                #count the common venue\n",
    "                same_count = 0\n",
    "                for v in ven.keys():\n",
    "                    paper_venue = train.iloc[i]['venue']\n",
    "                    if v == paper_venue:\n",
    "                        same_count += ven[v]\n",
    "\n",
    "                #add the features to the new_featurs\n",
    "                new_featurs['work_ratio'].append(work_ratio)\n",
    "                new_featurs['same_venue'].append(same_count)\n",
    "                new_featurs['coauthors_count'].append(coauthors_count)\n",
    "                new_featurs['prolific_count'].append(prolific_count)\n",
    "                if wad in true_lable:\n",
    "                    new_featurs['is_p_author'].append(1)\n",
    "                else:\n",
    "                    new_featurs['is_p_author'].append(0)\n",
    "\n",
    "            # print('all_prolific_authors', all_prolific_authors)\n",
    "            # print('the coauthors of paper', i, 'are', authors)\n",
    "            # print('worked authors dic', worked_authors_dic)\n",
    "            \n",
    "            print('== ',i,'======================='*5)\n",
    "    \n",
    "    #return the new features as pandas dataframe\n",
    "    new_featurs = pd.DataFrame(new_featurs)\n",
    "\n",
    "    #Save the data into a json file\n",
    "    new_featurs.to_json('new_featurs.json')\n",
    "\n",
    "    return new_featurs\n",
    "\n",
    "#create a logistic regression model\n",
    "def logistic_regression_model(X_train, y_train, penalty='l2', C=1.0, solver='liblinear', max_iter=100, l1_ratio=None):\n",
    "    logistic_regression = LogisticRegression(penalty, C, solver, max_iter, l1_ratio)\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    return logistic_regression\n",
    "\n",
    "# Determine whether the author is the author of the paper using logistic regression\n",
    "def is_prolific_author(author,paper_info):\n",
    "    authors = paper_info['coauthors']\n",
    "    venue = paper_info['venue']\n",
    "    return 1\n",
    "\n",
    "\n",
    "feature_engineering(X_train[0:1000], y_train[0:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fb6f22b9ea3684756870944b5f3fb664fed3c82022a08a9b2c6685fd75d4878"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
