{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from dataset and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#read train.json data\n",
    "orign_train_data = pd.read_json('train.json')\n",
    "\n",
    "#deep copy the original data\n",
    "train_data = copy.deepcopy(orign_train_data)\n",
    "\n",
    "#Get the authors list\n",
    "train_data_authors = train_data['authors']\n",
    "\n",
    "prolific_authors = []\n",
    "coauthors = []\n",
    "#Get the prolific authors list to train the model by removing the coauthors\n",
    "for author in train_data_authors:\n",
    "    p_authors = []\n",
    "    np_authors = []\n",
    "    for name in author:\n",
    "        if name < 100:\n",
    "            p_authors.append(name)\n",
    "        else:\n",
    "            np_authors.append(name)\n",
    "    prolific_authors.append(p_authors)\n",
    "    coauthors.append(np_authors)\n",
    "\n",
    "#add the prolific authors list to the train data\n",
    "train_data['coauthors'] = coauthors\n",
    "train_data['prolific_authors'] = prolific_authors\n",
    "\n",
    "#remove authors in the train data\n",
    "train_data = train_data.drop(['authors'], axis=1)\n",
    "\n",
    "p_a = train_data['prolific_authors']\n",
    "\n",
    "#read test.json data\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "#Pack the prediction result into a csv file\n",
    "def pack_result(result, file_name):\n",
    "    result = pd.DataFrame(result)\n",
    "    #change the 'ID' and 'Predict' column to int32\n",
    "    result['ID'] = result['ID'].astype('int32')\n",
    "    result['Predict'] = result['Predict'].astype('int32')\n",
    "    result.columns = ['ID', 'Predict']\n",
    "    result.to_csv(file_name, index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets and make the no prolific paper has the -1 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the train data into training set and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['prolific_authors'],axis=1), p_a, test_size=0.2, random_state=42)\n",
    "\n",
    "#get the same index entries in the original train data for training the model\n",
    "X_train_orgin = orign_train_data.loc[X_train.index]\n",
    "\n",
    "#re order the index of the X_train_orgin\n",
    "X_train_orgin = X_train_orgin.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the y_train\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the X_test\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "#re order the index of the y_test\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_train.iloc[i]) == 0:\n",
    "        y_train.iloc[i].append(-1)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    # if the author list is empty, add -1 to the list\n",
    "    if len(y_test.iloc[i]) == 0:\n",
    "        y_test.iloc[i].append(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that used to find information from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16336, 1762, 4357, 12564]\n",
      "[]\n",
      "[16336, 1762, 4357, 12564, 21189, 794, 2749, 19810, 15307, 16229, 15313, 15688, 1130, 3077, 19345, 10600, 7441, 2853, 2408, 805, 19908, 914, 16964, 935, 6301, 8842, 6165, 12639, 7539, 15378, 12891, 18730, 957, 16102, 20460, 4932, 5264, 20327, 17607, 12279, 9658, 3417, 7711, 16036, 4644, 15298, 11297, 10012, 16034, 3785, 3631, 14489, 16229, 16227, 1707, 11281, 12795, 10012, 2614, 13576, 15689, 19182, 17872, 20760, 1980, 19354, 16560, 1075, 3977, 283, 13756, 8142, 7021, 14987, 15645, 7098, 14284, 18463, 12772, 4971, 4695, 14883, 16814, 1323, 19990, 17160, 19330, 13081, 1846, 3830, 12279, 7540, 17022, 540, 994, 14773, 5974, 7669, 21148, 4004, 17849, 972, 14388, 8425, 4790, 9248, 1474, 9647, 6461, 890, 19267, 16137, 18927]\n"
     ]
    }
   ],
   "source": [
    "all_authors = X_train_orgin['authors']\n",
    "\n",
    "#find authors that author x used to worked with and how many times\n",
    "def find_author_x_work_with(author):\n",
    "    author_x_work_with = {}\n",
    "    author_x_paper = find_author_x_paper(author)\n",
    "    for paper in author_x_paper:\n",
    "        for ath in all_authors.loc[paper]:\n",
    "            if ath != author:\n",
    "                if ath in author_x_work_with:\n",
    "                    author_x_work_with[ath] += 1\n",
    "                else:\n",
    "                    author_x_work_with[ath] = 1\n",
    "    return author_x_work_with\n",
    "\n",
    "#find which paper that author x participated\n",
    "def find_author_x_paper(author):\n",
    "    author_x_paper = []\n",
    "    for i in range(len(all_authors)):\n",
    "        if author in all_authors.loc[i]:\n",
    "            author_x_paper.append(i)\n",
    "    return author_x_paper\n",
    "\n",
    "#find which paper that author x participated\n",
    "def find_author_x_paper(author):\n",
    "    author_x_paper = []\n",
    "    for i in X_train_orgin.index:\n",
    "        if author in X_train_orgin['authors'].loc[i]:\n",
    "            author_x_paper.append(i)\n",
    "    return author_x_paper\n",
    "\n",
    "#find the venue that author published papers\n",
    "def find_author_venue(author):\n",
    "    venues = {}\n",
    "    author_paper = find_author_x_paper(author)\n",
    "    for paper in author_paper:\n",
    "        venue = X_train_orgin.loc[paper]['venue']\n",
    "        if venue in venues:\n",
    "            venues[venue] += 1\n",
    "        else:\n",
    "            venues[venue] = 1\n",
    "    return venues\n",
    "\n",
    "\n",
    "#whether the author x and author y worked with each other\n",
    "def is_author_x_work_with_author_y(author_x, author_y):\n",
    "    author_x_paper = find_author_x_paper(author_x)\n",
    "    author_y_paper = find_author_x_paper(author_y)\n",
    "    for paper in author_x_paper:\n",
    "        if paper in author_y_paper:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "#find the prolific authors that the authors worked with (at least one of given authors worked with)\n",
    "def get_authors_prolific_atleast_one(authors):\n",
    "    prolific_authors = []\n",
    "    for author in authors:\n",
    "        prolific_authors.append(find_author_x_work_with(author))\n",
    "    worked_authors = []\n",
    "    for entry in prolific_authors:\n",
    "        worked_authors.extend(entry.keys())\n",
    "\n",
    "    #remove depulicate authors in worked_authors\n",
    "    if worked_authors == []:\n",
    "        return []\n",
    "    worked_authors = list(set(worked_authors))\n",
    "    #get the prolific authors that these authors all worked with\n",
    "    #prolific_authors = set.union(*map(set, prolific_authors))\n",
    "\n",
    "    p_a = []\n",
    "    for author in worked_authors:\n",
    "        if author < 100:\n",
    "            p_a.append(author)\n",
    "    return p_a\n",
    "\n",
    "print(test_data['coauthors'].iloc[0])\n",
    "print(find_author_x_paper(16336))\n",
    "\n",
    "#check whether the author x ever published papers without using find_author_x_paper function\n",
    "def is_author_x_published(author):\n",
    "\n",
    "    for i in range(len(all_authors)):\n",
    "        if author in X_train_orgin['authors'].loc[i]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "not_published = []\n",
    "#see how many coauthor in the test data never published papers\n",
    "for coauthor in test_data['coauthors'][0:100]:\n",
    "    for author in coauthor:\n",
    "        if not is_author_x_published(author):\n",
    "            not_published.append(author)\n",
    "\n",
    "print(not_published)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering process for the logistic regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def feature_engineering(train,test):\n",
    "    new_featurs = {}\n",
    "\n",
    "    #find the ratio of how many coauthors had worked with the prolific author\n",
    "    new_featurs['work_ratio'] = []\n",
    "\n",
    "    #see if the time of prolific author has worked with the author in the same venue\n",
    "    new_featurs['same_venue'] = []\n",
    "\n",
    "    #how many coauthors in the given data\n",
    "    new_featurs['coauthors_count'] = []\n",
    "\n",
    "    #how mant prolific authors that at least one of the coauthors has worked with\n",
    "    new_featurs['prolific_count'] = []\n",
    "\n",
    "    #whether this prolific author has participated in the same paper with the given coauthors\n",
    "    new_featurs['is_p_author'] = []\n",
    "\n",
    "    for i in range(len(train)):\n",
    "        authors = train.iloc[i]['coauthors']\n",
    "\n",
    "        #the paper that have only one author and it is also a prolific author will have no coauthors\n",
    "        if len(authors) != 0:\n",
    "\n",
    "            #create two dictionary to store the prolific authors and paper that the prolific authors have participated\n",
    "            worked_authors_dic = {}\n",
    "            published_paper_dic = {}\n",
    "            true_lable = test.iloc[i]\n",
    "\n",
    "            all_prolific_authors = get_authors_prolific_atleast_one(authors)\n",
    "\n",
    "            #these two features are same for all the possible prolific authors in the given paper\n",
    "            prolific_count = len(all_prolific_authors)\n",
    "            coauthors_count = len(authors)\n",
    "\n",
    "            if all_prolific_authors != []:\n",
    "                for p_author in all_prolific_authors:\n",
    "                    \n",
    "                    all_p_authors_worked_with = find_author_x_work_with(p_author)\n",
    "                    #copy the keys of the all_p_authors_worked_with\n",
    "                    all_p_authors_worked_with_keys = copy.deepcopy(list(all_p_authors_worked_with.keys()))\n",
    "\n",
    "                    for p_w_author in all_p_authors_worked_with_keys:\n",
    "                        #remove the authors that the author have not worked with\n",
    "                        if p_w_author not in authors:\n",
    "                            all_p_authors_worked_with.pop(p_w_author)\n",
    "\n",
    "                    worked_authors_dic[p_author] = all_p_authors_worked_with\n",
    "            \n",
    "            for wad in worked_authors_dic:\n",
    "                work_ratio = len(worked_authors_dic[wad])/coauthors_count\n",
    "                # print('author', wad, 'ratio', work_ratio)\n",
    "\n",
    "                # venue that prolific author has published paper\n",
    "                ven = find_author_venue(wad)\n",
    "\n",
    "                #count the common venue\n",
    "                same_count = 0\n",
    "                for v in ven.keys():\n",
    "                    paper_venue = train.iloc[i]['venue']\n",
    "                    if v == paper_venue:\n",
    "                        same_count += ven[v]\n",
    "\n",
    "                #add the features to the new_featurs\n",
    "                new_featurs['work_ratio'].append(work_ratio)\n",
    "                new_featurs['same_venue'].append(same_count)\n",
    "                new_featurs['coauthors_count'].append(coauthors_count)\n",
    "                new_featurs['prolific_count'].append(prolific_count)\n",
    "                if wad in true_lable:\n",
    "                    new_featurs['is_p_author'].append(1)\n",
    "                else:\n",
    "                    new_featurs['is_p_author'].append(0)\n",
    "            \n",
    "            print('== ',i,'======================='*5)\n",
    "    \n",
    "    #return the new features as pandas dataframe\n",
    "    new_featurs = pd.DataFrame(new_featurs)\n",
    "\n",
    "    #Save the data into a json file\n",
    "    new_featurs.to_json('new_featurs.json')\n",
    "\n",
    "    return new_featurs\n",
    "\n",
    "#create a logistic regression model\n",
    "def logistic_regression_model(X_train, y_train, penalty='l2', C=1.0, solver='liblinear', max_iter=100, l1_ratio=None):\n",
    "    logistic_regression = LogisticRegression()\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    return logistic_regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all the , and [] in the given csv file\n",
    "def csv_generate(csv_file,file_name):\n",
    "    prediction = pd.read_csv(csv_file)\n",
    "\n",
    "    #remove all the , and [] in the prediction1.csv\n",
    "    prediction['Predict'] = prediction['Predict'].str.replace('[','')\n",
    "    prediction['Predict'] = prediction['Predict'].str.replace(']','')\n",
    "    prediction['Predict'] = prediction['Predict'].str.replace(',','')\n",
    "\n",
    "    #save the prediction into a csv file\n",
    "    prediction.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier                                                    0\n",
      "coauthors                            [16336, 1762, 4357, 12564]\n",
      "year                                                         19\n",
      "abstract      [37, 1662, 3207, 10, 33, 2037, 1738, 1642, 155...\n",
      "venue                                                       223\n",
      "title         [3207, 24, 1798, 1738, 37, 2375, 1568, 11, 53,...\n",
      "Name: 0, dtype: object\n",
      "the length of the test data before removing the rows that have no coauthors 5079\n",
      "the length of the test data after removing the rows that have no coauthors 5079\n",
      "========== 0 ==========\n",
      "========== 1 ==========\n",
      "2 ratio:  0.5\n",
      "2 venue_count:  0\n",
      "6 ratio:  0.5\n",
      "6 venue_count:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m prolific_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(apa)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m#ratio calculation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m all_p_authors_worked_with \u001b[39m=\u001b[39m find_author_x_work_with(p_author)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m all_p_authors_worked_with_keys \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mlist\u001b[39m(all_p_authors_worked_with\u001b[39m.\u001b[39mkeys()))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m p_w_author \u001b[39min\u001b[39;00m all_p_authors_worked_with_keys:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     \u001b[39m#remove the authors that not in the coauthors list\u001b[39;00m\n",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb Cell 10\u001b[0m in \u001b[0;36mfind_author_x_work_with\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_author_x_work_with\u001b[39m(author):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     author_x_work_with \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     author_x_paper \u001b[39m=\u001b[39m find_author_x_paper(author)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m paper \u001b[39min\u001b[39;00m author_x_paper:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39mfor\u001b[39;00m ath \u001b[39min\u001b[39;00m all_authors\u001b[39m.\u001b[39mloc[paper]:\n",
      "\u001b[1;32m/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb Cell 10\u001b[0m in \u001b[0;36mfind_author_x_paper\u001b[0;34m(author)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m author_x_paper \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_authors)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m author \u001b[39min\u001b[39;00m all_authors\u001b[39m.\u001b[39;49mloc[i]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         author_x_paper\u001b[39m.\u001b[39mappend(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jc/Documents/GitHub/SML_Project2/SML_Project/SML_Project2.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m author_x_paper\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CV/lib/python3.10/site-packages/pandas/core/indexing.py:966\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 966\u001b[0m     maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39;49mapply_if_callable(key, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj)\n\u001b[1;32m    967\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/CV/lib/python3.10/site-packages/pandas/core/common.py:357\u001b[0m, in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_if_callable\u001b[39m(maybe_callable, obj, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    347\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    Evaluate possibly callable input using obj and kwargs if it is callable,\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m    otherwise return as it is.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39m    **kwargs\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mif\u001b[39;00m callable(maybe_callable):\n\u001b[1;32m    358\u001b[0m         \u001b[39mreturn\u001b[39;00m maybe_callable(obj, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m maybe_callable\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "new_features_1 = pd.read_json('features_data_0_5000.json')\n",
    "new_features_2 = pd.read_json('features_data_5000_10000.json')\n",
    "new_features_3 = pd.read_json('new_featurs_10000_20000.json')\n",
    "\n",
    "new_features_origin = pd.concat([new_features_1, new_features_2, new_features_3], ignore_index=True)\n",
    "#split the new_features where the is_p_author is 1 and 0\n",
    "new_features_true = new_features_origin[new_features_origin['is_p_author'] == 1]\n",
    "new_features_false = new_features_origin[new_features_origin['is_p_author'] == 0]\n",
    "\n",
    "\n",
    "#add same number of false data to the true data (1:1)\n",
    "training_data_1_1 = pd.concat([new_features_true, new_features_false.sample(n=len(new_features_true), random_state=1)], ignore_index=True)\n",
    "\n",
    "#add twice as many false data to the true data (1:2)\n",
    "training_data_1_2 = pd.concat([new_features_true, new_features_false.sample(n=len(new_features_true)*2, random_state=1)], ignore_index=True)\n",
    "\n",
    "#add half as many false data to the true data (1:0.5)\n",
    "training_data_1_05 = pd.concat([new_features_true, new_features_false.sample(n=int(len(new_features_true)/2), random_state=1)], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#shuffle the data\n",
    "training_data_1_1 = shuffle(training_data_1_1)\n",
    "training_data_1_2 = shuffle(training_data_1_2)\n",
    "training_data_1_05 = shuffle(training_data_1_05)\n",
    "\n",
    "#training the logistic regression model using the original data\n",
    "X_train_lr = new_features_origin[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n",
    "y_train_lr = new_features_origin['is_p_author']\n",
    "\n",
    "#training the logistic regression model using the training_data_1_1\n",
    "X_train_lr_11 = training_data_1_1[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n",
    "y_train_lr_11 = training_data_1_1['is_p_author']\n",
    "\n",
    "#training the logistic regression model using the training_data_1_2\n",
    "X_train_lr_12 = training_data_1_2[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n",
    "y_train_lr_12 = training_data_1_2['is_p_author']\n",
    "\n",
    "#training the logistic regression model using the training_data_1_05\n",
    "X_train_lr_105 = training_data_1_05[['work_ratio', 'same_venue', 'coauthors_count', 'prolific_count']]\n",
    "y_train_lr_105 = training_data_1_05['is_p_author']\n",
    "\n",
    "\n",
    "LR = logistic_regression_model(X_train_lr, y_train_lr)\n",
    "LR11 = logistic_regression_model(X_train_lr_11, y_train_lr_11)\n",
    "LR12 = logistic_regression_model(X_train_lr_12, y_train_lr_12)\n",
    "LR105 = logistic_regression_model(X_train_lr_105, y_train_lr_105)\n",
    "\n",
    "print(test_data.iloc[0])\n",
    "#predict the test data\n",
    "prediction = {}\n",
    "prediction['ID'] = []\n",
    "prediction['Predict'] = []\n",
    "\n",
    "#remove all the rows that have no coauthors\n",
    "print('the length of the test data before removing the rows that have no coauthors', len(X_test))\n",
    "X_test = X_test[X_test['coauthors'].map(len) != 0]\n",
    "#remove the same index in the y_test\n",
    "y_test = y_test[X_test.index]\n",
    "print('the length of the test data after removing the rows that have no coauthors', len(X_test))\n",
    "\n",
    "start = 0\n",
    "end = len(test_data)\n",
    "data_to_predict = test_data[start:end]\n",
    "label_to_predict = y_test[start:end]\n",
    "select_model = LR105\n",
    "\n",
    "for i in range(len(data_to_predict)):\n",
    "    authors = data_to_predict.iloc[i]['coauthors']\n",
    "    prediction['ID'].append(i)\n",
    "\n",
    "    #all possible prolific authors of the paper\n",
    "    apa = get_authors_prolific_atleast_one(authors)\n",
    "    print('='*10, i, '='*10)\n",
    "    if apa != []:\n",
    "        prediction_ls = []\n",
    "        for p_author in apa:\n",
    "            ratio = 0\n",
    "            venue_count = 0\n",
    "            coauthors_count = len(authors)\n",
    "            prolific_count = len(apa)\n",
    "\n",
    "            #ratio calculation\n",
    "            all_p_authors_worked_with = find_author_x_work_with(p_author)\n",
    "            all_p_authors_worked_with_keys = copy.deepcopy(list(all_p_authors_worked_with.keys()))\n",
    "            for p_w_author in all_p_authors_worked_with_keys:\n",
    "                #remove the authors that not in the coauthors list\n",
    "                if p_w_author not in authors:\n",
    "                    all_p_authors_worked_with.pop(p_w_author)\n",
    "            ratio = len(all_p_authors_worked_with)/coauthors_count\n",
    "            print(p_author,'ratio: ',ratio)\n",
    "            #venue calculation\n",
    "            paper_venue = data_to_predict.iloc[i]['venue']\n",
    "            p_author_venue = find_author_venue(p_author)\n",
    "            if paper_venue in p_author_venue.keys():\n",
    "                for key in p_author_venue.keys():\n",
    "                    if key == paper_venue:\n",
    "                        venue_count = venue_count + 1\n",
    "                print(p_author,'venue_count: ',venue_count)\n",
    "            else:\n",
    "                venue_count = 0\n",
    "\n",
    "            print(p_author,'venue_count: ',venue_count)\n",
    "            result = select_model.predict([[ratio, venue_count, coauthors_count, prolific_count]])\n",
    "            if result == [1]:\n",
    "                print('the author is the possible author of the paper', p_author)\n",
    "                print('ratio', ratio)\n",
    "                prediction_ls.append(p_author)\n",
    "\n",
    "        #if no prolific author is a good candidate, then predict no prolific author\n",
    "        if prediction_ls != []: \n",
    "            prediction['Predict'].append(prediction_ls)\n",
    "        else:\n",
    "            prediction['Predict'].append([-1])\n",
    "\n",
    "    else:\n",
    "        prediction['Predict'].append([-1])\n",
    "\n",
    "print(prediction['Predict'])\n",
    "\n",
    "#calculate the accuracy using sklearn\n",
    "prediction = pd.DataFrame(prediction)\n",
    "\n",
    "#save the prediction into a csv file\n",
    "\n",
    "prediction.to_csv('test_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('CV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9563c21e2246d30c17eee537fc82eb5a8b28d7639b48b7c9a9fa6253fd64ec2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
